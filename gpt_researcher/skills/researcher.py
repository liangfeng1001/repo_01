import asyncio
import random
import json
import re
import html
from typing import Dict, Optional, List, Any, AsyncGenerator
import logging
import os
from gpt_researcher.llm_provider.generic.base import GenericLLMProvider
from ..actions.utils import stream_output
from ..actions.query_processing import plan_research_outline, get_search_results, generate_pubmed_sub_queries
from ..document import DocumentLoader, OnlineDocumentLoader, LangChainDocumentLoader
from ..utils.enum import ReportSource, ReportType, Tone
from ..utils.logging_config import get_json_handler, get_research_logger
from pathlib import Path
import markdown
from weasyprint import HTML
import openai
import numpy as np
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests
import pandas as pd
from ..retrievers import PubmedDianSearch


authors_t = 1

class ResearchConductor:
    """Manages and coordinates the research process."""

    def __init__(self, researcher):
        self.researcher = researcher
        self.logger = logging.getLogger('research')
        self.json_handler = get_json_handler()

    async def plan_research(self, query, query_domains=None):
        self.logger.info(f"Planning research for query: {query}")
        if query_domains:
            self.logger.info(f"Query domains: {query_domains}")
        
        await stream_output(
            "logs",
            "planning_research",
            f"üåê Browsing the web to learn more about the task: {query}...",
            self.researcher.websocket,
        )

        search_results = await get_search_results(query, self.researcher.retrievers[0], query_domains)
        self.logger.info(f"Initial search results obtained: {len(search_results)} results")

        await stream_output(
            "logs",
            "planning_research",
            f"ü§î Planning the research strategy and subtasks...",
            self.researcher.websocket,
        )


        outline = await plan_research_outline(
            query=query,
            search_results=search_results,
            agent_role_prompt=self.researcher.role,
            cfg=self.researcher.cfg,
            parent_query=self.researcher.parent_query,
            report_type=self.researcher.report_type,
            cost_callback=self.researcher.add_costs
        )
        self.logger.info(f"Research outline planned: {outline}")
        return outline

    async def conduct_research(self):
        """Runs the GPT Researcher to conduct research"""
        if self.json_handler:
            self.json_handler.update_content("query", self.researcher.query)
        
        self.logger.info(f"Starting research for query: {self.researcher.query}")
        self.logger.info(f"Report type: {self.researcher.report_type}")
        
        # Â¶ÇÊûúÊòØ‰∏ªÊü•ËØ¢ÔºàÈùûÂ≠ê‰∏ªÈ¢òÊä•ÂëäÔºâÔºåÈáçÁΩÆÁªìÊûú
        if self.researcher.report_type != "subtopic_report":
            self.logger.info(f"Main query detected, resetting accumulated results")

            # self.researcher.accumulated_classified_results = {
            #     "arxiv": [],
            #     "pubmed": [],
            #     "tavily": []
            # }

            #todo ‰øÆ+ÔºàËØïÔºâ
            self.researcher.accumulated_classified_results = {}



        else:
            self.logger.info(f"Subtopic report detected, using existing accumulated results")
            if not hasattr(self.researcher, 'accumulated_classified_results'):
                self.logger.error("No existing accumulated results found for subtopic report")
                raise AttributeError("accumulated_classified_results not found for subtopic report")
        
        # Reset visited_urls and source_urls at the start of each research task
        self.researcher.visited_urls.clear()
        research_data = []

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "starting_research",
                f"üîç Starting the research task for '{self.researcher.query}'...",
                self.researcher.websocket,
            )

        if self.researcher.verbose:
            await stream_output("logs", "agent_generated", self.researcher.agent, self.researcher.websocket)

        # Research for relevant sources based on source types below
        if self.researcher.source_urls:
            self.logger.info("Using provided source URLs")
            research_data = await self._get_context_by_urls(self.researcher.source_urls)
            if research_data and len(research_data) == 0 and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "answering_from_memory",
                    f"üßê I was unable to find relevant context in the provided sources...",
                    self.researcher.websocket,
                )
            if self.researcher.complement_source_urls:
                self.logger.info("Complementing with web search")
                additional_research = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
                research_data += ' '.join(additional_research)

        elif self.researcher.report_source == ReportSource.Web.value:
            self.logger.info("Using web search")
            research_data = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains, self.researcher.accumulated_classified_results)

        # ... rest of the conditions ...
        elif self.researcher.report_source == ReportSource.Local.value:
            self.logger.info("Using local search")
            document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            self.logger.info(f"Loaded {len(document_data)} documents")
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)

            research_data = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)

        # Hybrid search including both local documents and web sources
        elif self.researcher.report_source == ReportSource.Hybrid.value:
            if self.researcher.document_urls:
                document_data = await OnlineDocumentLoader(self.researcher.document_urls).load()
            else:
                document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)
            docs_context = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)
            web_context = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
            research_data = f"Context from local documents: {docs_context}\n\nContext from web sources: {web_context}"

        elif self.researcher.report_source == ReportSource.Azure.value:
            from ..document.azure_document_loader import AzureDocumentLoader
            azure_loader = AzureDocumentLoader(
                container_name=os.getenv("AZURE_CONTAINER_NAME"),
                connection_string=os.getenv("AZURE_CONNECTION_STRING")
            )
            azure_files = await azure_loader.load()
            document_data = await DocumentLoader(azure_files).load()  # Reuse existing loader
            research_data = await self._get_context_by_web_search(self.researcher.query, document_data)
            
        elif self.researcher.report_source == ReportSource.LangChainDocuments.value:
            langchain_documents_data = await LangChainDocumentLoader(
                self.researcher.documents
            ).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(langchain_documents_data)
            research_data = await self._get_context_by_web_search(
                self.researcher.query, langchain_documents_data, self.researcher.query_domains
            )

        elif self.researcher.report_source == ReportSource.LangChainVectorStore.value:
            research_data = await self._get_context_by_vectorstore(self.researcher.query, self.researcher.vector_store_filter)

        # Rank and curate the sources
        self.researcher.context = research_data
        if self.researcher.cfg.curate_sources:
            self.logger.info("Curating sources")
            self.researcher.context = await self.researcher.source_curator.curate_sources(research_data)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "research_step_finalized",
                f"Finalized research step.\nüí∏ Total Research Costs: ${self.researcher.get_costs()}",
                self.researcher.websocket,
            )
            if self.json_handler:
                self.json_handler.update_content("costs", self.researcher.get_costs())
                self.json_handler.update_content("context", self.researcher.context)

        self.logger.info(f"Research completed. Context size: {len(str(self.researcher.context))}")
        return self.researcher.context

    async def _get_context_by_urls(self, urls):
        """Scrapes and compresses the context from the given urls"""
        self.logger.info(f"Getting context from URLs: {urls}")
        
        new_search_urls = await self._get_new_urls(urls)
        self.logger.info(f"New URLs to process: {new_search_urls}")

        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)
        self.logger.info(f"Scraped content from {len(scraped_content)} URLs")

        if self.researcher.vector_store:
            self.logger.info("Loading content into vector store")
            self.researcher.vector_store.load(scraped_content)

        context = await self.researcher.context_manager.get_similar_content_by_query(
            self.researcher.query, scraped_content
        )
        return context

    # Add logging to other methods similarly...

    async def _get_context_by_vectorstore(self, query, filter: Optional[dict] = None):
        """
        Generates the context for the research task by searching the vectorstore
        Returns:
            context: List of context
        """
        self.logger.info(f"Starting vectorstore search for query: {query}")
        context = []
        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query)
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"üóÇÔ∏è  I will conduct my research based on the following queries: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # Using asyncio.gather to process the sub_queries asynchronously
        context = await asyncio.gather(
            *[
                self._process_sub_query_with_vectorstore(sub_query, filter)
                for sub_query in sub_queries
            ]
        )
        return context

    async def _calculate_query_similarity(self, main_query, sub_query):
        """ËÆ°ÁÆó‰∏ªÊü•ËØ¢ÂíåÂ≠êÊü•ËØ¢ÁöÑÁõ∏‰ººÂ∫¶"""
        main_embedding = await self.researcher.memory.get_embeddings().aembed_query(main_query)
        sub_embedding = await self.researcher.memory.get_embeddings().aembed_query(sub_query)
        similarity = np.dot(main_embedding, sub_embedding) / (
            np.linalg.norm(main_embedding) * np.linalg.norm(sub_embedding)
        )
        return similarity

    def _calculate_context_rank_score(self, total_results, current_rank):
        """ËÆ°ÁÆó‰∏ä‰∏ãÊñáÊéíÂ∫èÂæóÂàÜ"""
        return (total_results - current_rank + 1) / total_results

    def _calculate_source_authority_score(self, source):
        """ËÆ°ÁÆóÊù•Ê∫êÊùÉÂ®ÅÊÄßÂæóÂàÜ"""
        source_scores = {
            "pubmed": 5,
            "arxiv": 3,
            "tavily": 2
        }
        return source_scores.get(source.lower(), 0)

    async def _get_context_by_web_search(self, query, scraped_data: list = [], query_domains: list = [], accumulated_classified_results: dict = None):
        """
        Generates the context for the research task by searching the query and scraping the results
        Returns:
            context: List of context
        """
        self.logger.info(f"Starting web search for query: {query}")
        
        # ËΩΩÂÖ•ÊúüÂàäÂΩ±ÂìçÂõ†Â≠êExcelÊï∞ÊçÆ
        journal_df = None
        excel_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "data/journal_impact_factors.xlsx")
        try:
            journal_df = pd.read_excel(excel_path)
            self.logger.info(f"Successfully loaded journal impact factors from {excel_path}")
            self.logger.info(f"Loaded {len(journal_df)} journal entries")
        except Exception as e:
            self.logger.warning(f"Could not load journal impact factors: {e}. Will use default scores.")
        
        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query, query_domains)
        self.logger.info(f"Generated sub-queries: {sub_queries}")
        
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            mark_query = "\n" + "".join([f"## {q.strip()}\n" for q in sub_queries])
            await stream_output(
                "logs",
                "mark_query",
                f"# {query}{mark_query}",
                self.researcher.websocket,
            )
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"üóÇÔ∏è I will conduct my research based on the following queries: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # ÊµãËØïÁî®Ôºö
        self.logger.info(f"ÂÖ®ÈÉ®ÁöÑÊü•ËØ¢queryÁöÑÂÜÖÂÆπÊòØÔºö{sub_queries}")

        # Ëé∑ÂèñÂéüÂßãÊêúÁ¥¢ÁªìÊûú
        context = await asyncio.gather(
            *[
                self._process_sub_query(sub_query, scraped_data, query_domains)
                for sub_query in sub_queries
            ]
        )


        # Âä†+
        min_jif = self.researcher.min_jif_score  # ÈúÄÊ∑ªÂä†ÊúÄ‰ΩéÊúüÂæÖÂΩ±ÂìçÂõ†Â≠êÂæóÂàÜÁöÑÂ≠óÊÆµ

        #todo ‰øÆ+
        source_type_keys = ["tavily", "arxiv", "pubmed"]  # ÂÆö‰πâÊù•Ê∫êÁ±ªÂûãÈîÆ

        # ËÆ∞ÂΩïÂéüÂßãÊêúÁ¥¢ÁªìÊûú
        self.logger.info("raw search results:")
        for idx, contents in enumerate(context):
            if contents:
                self.logger.info(f"subquery {idx + 1} - '{sub_queries[idx]}' research results:")
                # Â∞ÜcontentsÊåâÂùóÂàÜÂâ≤
                content_blocks = contents.split("\n\n")
                self.logger.info(f"find {len(content_blocks)} content_blocks")
                self.logger.info("-" * 50)
        
        # Â§ÑÁêÜÊâÄÊúâÊêúÁ¥¢ÁªìÊûúÂπ∂ËÆ°ÁÆóÂæóÂàÜ
        try:
            all_scored_items = []

            # Áõ¥Êé•ÈÅçÂéÜcontextÂàóË°®
            for idx, contents in enumerate(context):
                if not contents:
                    continue
                
                # Â∞ÜÂÜÖÂÆπÊåâÂùóÂàÜÂâ≤
                content_blocks = contents.split("\n\n")

                # ÈÅçÂéÜËØ•Êü•ËØ¢ÁöÑÊâÄÊúâÂÜÖÂÆπÂùó
                for block in content_blocks:
                    if not block.strip():
                        continue

                    # Ëß£ÊûêÂùóÂÜÖÂÆπ
                    source_match = re.search(r'^Source: (https?://[^\s]+)', block, re.M)
                    title_match = re.search(r'Title: (.+)', block)
                    content_match = re.search(r'Content: (.+)', block, re.DOTALL)
                    
                    if not all([source_match, title_match, content_match]):
                        continue
                        
                    url = source_match.group(1)
                    title = title_match.group(1).strip()
                    content_text = content_match.group(1).strip()



                    # Á°ÆÂÆöÊù•Ê∫êÈªòËÆ§Á±ªÂûã
                    source_type = source_type_keys[0]

                    # ‰ªéÂÜÖÂÆπÂùó‰∏≠Ëé∑ÂèñÊ£ÄÁ¥¢Âô®Á±ªÂûã
                    retriever_type_match = re.search(r'RetrieverType: (\w+)', block)
                    if retriever_type_match:
                        source_type = retriever_type_match.group(1)
                        self.logger.info(f"get source_type from content_block: {source_type}")  # ÊâìÂç∞Ê£ÄÁ¥¢Âô®Á±ªÂûã
                    else:
                        # ‰ªéscraped_data‰∏≠Ëé∑ÂèñÊ£ÄÁ¥¢Âô®Á±ªÂûã
                        for item in scraped_data:
                            if item.get('url') == url and 'retriever_type' in item:  # Ê£ÄÊü•URLÂíåÊ£ÄÁ¥¢Âô®Á±ªÂûãÊòØÂê¶Â≠òÂú®
                                source_type = item['retriever_type']  # Ëé∑ÂèñÊ£ÄÁ¥¢Âô®Á±ªÂûã
                                self.logger.info(f"get source_type from scraped_data: {source_type}")
                                break

                    # ‰øÆ+
                    # Ê£ÄÊü•Êù•Ê∫êÁ±ªÂûãÊòØÂê¶ÊúâÊïà
                    if source_type not in source_type_keys:  # Â¶ÇÊûúÊù•Ê∫êÁ±ªÂûãÊó†ÊïàÔºåÂàôË∑≥ËøáÊàñËÄÖÂº∫Âà∂‰øÆÊîπ‰∏∫ÈªòËÆ§Á±ªÂûã
                        # continue
                        # Âº∫Âà∂‰øÆÊîπ‰∏∫ÈªòËÆ§Á±ªÂûã
                        source_type = source_type_keys[0]  # ÈªòËÆ§Á±ªÂûã
                        self.logger.info(
                            f"Invalid source_type: {source_type}. Forcing to default: {source_type_keys[0]}")


                    # Â¶ÇÊûúÊù•Ê∫êÁ±ªÂûãÊòØtavilyÔºåÊ†πÊçÆURLÁâπÂæÅËøõË°å‰∫åÊ¨°ÂàÜÁ±ª
                    if source_type == source_type_keys[0]: # ‰øÆ+
                        if 'arxiv' in url.lower():  # Â¶ÇÊûúURLÂåÖÂê´arxivÔºåÂàôÂ∞ÜÊù•Ê∫êÁ±ªÂûãÊîπ‰∏∫arxiv
                            source_type = "arxiv"
                            self.logger.info("according to url, change source_type from tavily to arxiv")
                        elif 'ncbi' in url.lower() or 'pubmed' in url.lower():  # Â¶ÇÊûúURLÂåÖÂê´ncbiÊàñpubmedÔºåÂàôÂ∞ÜÊù•Ê∫êÁ±ªÂûãÊîπ‰∏∫pubmed
                            source_type = "pubmed"
                            self.logger.info("according to url, change source_type from tavily to pubmed")

                    # ËÆ°ÁÆóÊù•Ê∫êÊùÉÂ®ÅÊÄßÂæóÂàÜ
                    source_authority_score = self._calculate_source_authority_score(source_type)
                    
                    # ËÆ°ÁÆó‰∏ä‰∏ãÊñáÊéíÂ∫èÂæóÂàÜ - ‰ΩøÁî®ÂΩìÂâçÁªìÊûúÂú®ÊâÄÊúâÁªìÊûú‰∏≠ÁöÑ‰ΩçÁΩÆ
                    context_rank_score = self._calculate_context_rank_score(len(all_scored_items) + 1, len(all_scored_items))

                    # ËÆ°ÁÆóÂÜÖÂÆπ‰∏éÊü•ËØ¢ÁöÑÁõ∏‰ººÂ∫¶
                    content_similarity = await self._calculate_query_similarity(query, sub_queries[idx])  

                    impact_factor = 0


                    self.logger.info(f'our min_jif_score is {min_jif}')


                    journal_name = ""
                    if source_type == "pubmed" or source_type == "tavily":
                        # ÊèêÂèñÊúüÂàä‰ø°ÊÅØ
                        journal_info = await self._extract_journal_info_from_url(url)
                        journal_name = journal_info.get("journal_name", "")

                        self.logger.info(f"our journal_name is <<{journal_name}>> in this time")
                        # Êü•ÊâæÊúüÂàäÂΩ±ÂìçÂõ†Â≠ê
                        if journal_df is not None and journal_name:
                            try:
                                self.logger.info("We are starting to search for journal factors ")
                                # Ê†áÂáÜÂåñÊü•ËØ¢ÁöÑÊúüÂàäÂêçÁß∞
                                normalized_journal_name = await self._normalize_journal_name(journal_name)

                                # Â∞ùËØïÁ≤æÁ°ÆÂåπÈÖç
                                # È¶ñÂÖàÂàõÂª∫‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑÊúüÂàäÂêçÁß∞Âàó
                                journal_df['NormalizedName'] = journal_df['Name'].apply(
                                    lambda x: str(x).replace(' - ', '-').replace(' And ', ' & ').replace(' and ', ' & ').replace('&amp;', '&').replace('&AMP;', '&') if not pd.isna(x) else ""
                                )
                                journal_match = journal_df[journal_df['NormalizedName'].str.upper() == normalized_journal_name.upper()]



                                # Â¶ÇÊûúÊ≤°ÊúâÁ≤æÁ°ÆÂåπÈÖçÔºåÂ∞ùËØïÈÉ®ÂàÜÂåπÈÖç
                                if journal_match.empty:

                                    self.logger.info("Liang__is trying„Äã„Äã„Äã„Äã journal_math is also empty")

                                    for _, row in journal_df.iterrows():
                                        normalized_db_name = row['NormalizedName'].upper()
                                        
                                        if normalized_db_name in normalized_journal_name.upper() or normalized_journal_name.upper() in normalized_db_name:
                                            journal_match = pd.DataFrame([row])
                                            break
                                            
                                        # ‰πüÊ£ÄÊü•Áº©ÂÜôÂêç
                                        if 'AbbrName' in row and not pd.isna(row['AbbrName']):
                                            abbr_name = str(row['AbbrName']).upper()
                                            normalized_abbr_name = await self._normalize_journal_name(abbr_name)
                                            
                                            if normalized_abbr_name in normalized_journal_name.upper() or normalized_journal_name.upper() in normalized_abbr_name:
                                                journal_match = pd.DataFrame([row])
                                                break
                                
                                # Â¶ÇÊûúÊúâISSNÂåπÈÖç
                                if journal_match.empty and 'issn' in journal_info:

                                    self.logger.info("ÊúüÂàäÂåπÈÖç‰æùÁÑ∂Ê≤°ÊúâÊàêÂäüÔºÅÔºÅ")

                                    issn = journal_info['issn']
                                    journal_match = journal_df[(journal_df['ISSN'] == issn) | (journal_df['EISSN'] == issn)]
                                
                                if not journal_match.empty:

                                    impact_factor = float(journal_match.iloc[0]['JIF']) if 'JIF' in journal_match.columns and not pd.isna(journal_match.iloc[0]['JIF']) else 0
                                    self.logger.info(f"ÊúüÂàä '{journal_name}' ÁöÑÂΩ±ÂìçÂõ†Â≠ê (JIF): {impact_factor}")
                            except Exception as e:
                                self.logger.warning(f"ÊúüÂàäÂΩ±ÂìçÂõ†Â≠êÊü•ËØ¢ÈîôËØØ: {e}")


                    if impact_factor < min_jif:  # Â¶ÇÊûúÂΩ±ÂìçÂõ†Â≠êÂ∞è‰∫éÊúÄ‰ΩéÊúüÂæÖÂΩ±ÂìçÂõ†Â≠êÔºåÂàôË∑≥Ëøá
                       continue


                    # Ëé∑ÂèñÂØπÂ∫îÊñáÁåÆÁöÑÂèëË°®Âπ¥„ÄÅÂç∑Êúü„ÄÅÈ°µÁ†ÅÔºàËã•Ê≤°ÊúâÔºåÂè™ÁªôÂπ¥‰ªΩÂ∞±Â•ΩÔºâ
                    published_date = ""  # ÂàùÂßãÂåñÂèëË°®Êó•Êúü
                    authors = ""  # ÂàùÂßãÂåñ‰ΩúËÄÖ
                    vol = ""  # ÂàùÂßãÂåñÂç∑Âè∑
                    pagination = ""  # ÂàùÂßãÂåñÈ°µÁ†Å
                    if source_type == source_type_keys[2]:  # Â¶ÇÊûúÊù•Ê∫êÁ±ªÂûãÊòØpubmed

                        published_date_match = re.search(r'Published Date: (.+)', block)  # ÂåπÈÖçÂèëË°®Êó•Êúü
                        vol_match = re.search(r'Volume: (.+)', block)  # ÂåπÈÖçÂç∑Âè∑
                        pagination_match = re.search(r'Pagination: (.+)', block)  # ÂåπÈÖçÈ°µÁ†Å
                        authors_match = re.search(r'Authors: (.+)', block)  # ÂåπÈÖç‰ΩúËÄÖ
                        # ÊèêÂèñ‰ø°ÊÅØ
                        if authors_match:
                            self.logger.info("‰ΩúËÄÖÂåπÈÖç‰∫ÜÔºÅÔºÅ")
                            authors = authors_match.group(1).strip()  # Ëé∑Âèñ‰ΩúËÄÖ
                        if pagination_match:
                            self.logger.info("È°µÁ†ÅÂåπÈÖç‰∫ÜÔºÅÔºÅ")
                            pagination = pagination_match.group(1).strip()  # Ëé∑ÂèñÈ°µÁ†Å
                        if vol_match:
                            self.logger.info("Âç∑Âè∑ÂåπÈÖç‰∫ÜÔºÅÔºÅ")
                            vol = vol_match.group(1).strip()  # Ëé∑ÂèñÂç∑Âè∑
                        if published_date_match:
                            self.logger.info("Êó•ÊúüÂåπÈÖç‰∫ÜÔºÅÔºÅ")
                            published = published_date_match.group(1).strip()  # Ëé∑ÂèñÂèëË°®Êó•Êúü
                            # published_date = published.split(" ")[0] # ÊèêÂèñÂèëË°®Êó•Êúü
                            published_date = published[:4]

                    elif source_type == source_type_keys[1]:  # Â¶ÇÊûúÊù•Ê∫êÁ±ªÂûãÊòØarxiv
                        arxiv_info = self._extract_arxiv_info_from_url(url)  # ÊèêÂèñarxiv‰ø°ÊÅØ
                        published_date = arxiv_info.get("published_date", "")  # Ëé∑ÂèñÂèëË°®Êó•Êúü
                        # # Ê£ÄÊü•ÂèëË°®Êó•ÊúüÊòØÂê¶Âú®2020Âπ¥‰πãÂêé
                        # if published_date and int(published_date.split(".")[0]) < 2020:  # Ê£ÄÊü•ÂèëË°®Êó•ÊúüÊòØÂê¶Âú®2020Âπ¥‰πãÂêé
                        #     continue  # Â¶ÇÊûú‰∏çÊòØÔºåÂàôË∑≥Ëøá
                        authors = arxiv_info.get("arxiv_id", "Unknown ID")  # Ëé∑Âèñ‰ΩúËÄÖÔºàÂÖàÁî®ID‰ª£ÊõøÔºâ


                    if vol:
                        if pagination:  # Â¶ÇÊûúÊúâÂç∑Âè∑ÂíåÈ°µÁ†Å
                            vol_pagination = f"Vol.{vol}:{pagination}"  # ÊãºÊé•Âç∑Âè∑ÂíåÈ°µÁ†Å
                        else:  # Â¶ÇÊûúÂè™ÊúâÂç∑Âè∑
                            vol_pagination = f"Vol.{vol}"  # ÊãºÊé•Âç∑Âè∑
                    else:
                        vol_pagination = ""

                        # Ê†áÂáÜÂåñÂΩ±ÂìçÂõ†Â≠êÂæóÂàÜÂà∞0-1ËåÉÂõ¥
                    # ÂÅáËÆæÊúÄÈ´òÂΩ±ÂìçÂõ†Â≠ê‰∏∫100ÔºàÂèØÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµË∞ÉÊï¥Ôºâ
                    max_impact_factor = 503.1
                    normalized_impact_factor = min(impact_factor / max_impact_factor, 1.0)
                    
                    # ËÆ°ÁÆóÊÄªÂæóÂàÜ (Ë∞ÉÊï¥ÊùÉÈáç‰ª•ÂèçÊò†Êñ∞ÁöÑËØÑÂàÜÊñπÂºè)
                    total_score = (
                        0.3 * content_similarity +      # ÂÜÖÂÆπ‰∏éÊü•ËØ¢ÁöÑÁõ∏‰ººÂ∫¶
                        0.2 * context_rank_score +     # ‰∏ä‰∏ãÊñáÊéíÂ∫èÂæóÂàÜ
                        0.2 * source_authority_score + # Êù•Ê∫êÊùÉÂ®ÅÊÄßÂæóÂàÜ
                        0.3 * normalized_impact_factor # ÊúüÂàäÂΩ±ÂìçÂõ†Â≠êÂæóÂàÜ
                    )
                    
                    self.logger.info(f"***the content is from: {source_type}***")
                    all_scored_items.append({
                        'content': content_text,
                        'source': url,
                        'title': title,
                        'journal_name': journal_name,
                        'source_type': source_type,
                        'similarity_score': content_similarity,  # Êõ¥Êñ∞‰∏∫ÂÜÖÂÆπÁõ∏‰ººÂ∫¶
                        'context_rank_score': context_rank_score,
                        'source_authority_score': source_authority_score,
                        'impact_factor': impact_factor,
                        'normalized_impact_factor': normalized_impact_factor,
                        'score': total_score,
                        'published_date': published_date,   # ÂèëË°®Êó•Êúü
                        'authors': authors,   # ‰ΩúËÄÖ
                        'vol_pagination': vol_pagination,   # Âç∑Âè∑+È°µÁ†Å

                    })
                
            # ËÆ∞ÂΩïÊéíÂ∫èÂâçÁöÑÂæóÂàÜÊÉÖÂÜµ
            # self.logger.info("\nÊéíÂ∫èÂâçÁöÑÂæóÂàÜÊÉÖÂÜµ:")
            # for idx, item in enumerate(all_scored_items):
            #     try:
            #         self.logger.info(f"  item {idx + 1}:")
            #         self.logger.info(f"  url_source: {item['source']}")
            #         self.logger.info(f"  journal_name: {item['journal_name']}")
            #         self.logger.info(f"  source_type: {item['source_type']}")
            #         # Â§ÑÁêÜÊ†áÈ¢ò‰∏≠ÁöÑÁâπÊÆäÂ≠óÁ¨¶
            #         title = item['title'].encode('utf-8', errors='ignore').decode('utf-8')
            #         self.logger.info(f"  Ê†áÈ¢ò: {title}")
            #         self.logger.info(f"  ÂÜÖÂÆπÈïøÂ∫¶: {len(item['content'])}")
            #         self.logger.info(f"  Áõ∏‰ººÂ∫¶ÂæóÂàÜ: {item['similarity_score']:.4f}")
            #         self.logger.info(f"  ‰∏ä‰∏ãÊñáÊéíÂêçÂæóÂàÜ: {item['context_rank_score']:.4f}")
            #         self.logger.info(f"  Êù•Ê∫êÊùÉÂ®ÅÊÄßÂæóÂàÜ: {item['source_authority_score']:.4f}")
            #         self.logger.info(f"  ÂΩ±ÂìçÂõ†Â≠ê: {item['impact_factor']}")
            #         self.logger.info(f"  Ê†áÂáÜÂåñÂΩ±ÂìçÂõ†Â≠êÂæóÂàÜ: {item['normalized_impact_factor']:.4f}")
            #         self.logger.info(f"  ÊÄªÂàÜ: {item['score']:.4f}")
            #         self.logger.info("-" * 50)
            #     except Exception as e:
            #         self.logger.warning(f"ËÆ∞ÂΩïÈ°πÁõÆ {idx + 1} Êó∂Âá∫Èîô: {str(e)}")
            #         continue

            # ÊµãËØïÁî®Ôºö
            self.logger.info(f'Êú¨Ê¨°ÁöÑÂΩ±ÂìçÂõ†Â≠êÊúÄ‰ΩéË¶ÅÊ±ÇÊòØÔºö{min_jif}')
            self.logger.info(f'ÊÄªÂÖ±{len(all_scored_items)}‰∏™Ë¢´Êî∂ÂΩïÁöÑÊñáÁåÆ')



            # ÊåâÂæóÂàÜÊéíÂ∫è
            all_scored_items.sort(key=lambda x: x['score'], reverse=True)

            # self.logger.info(f'Â±ïÁ§∫Ë¢´Êî∂ÂΩïÈááÁ∫≥ÁöÑÊñáÁåÆ‰ø°ÊÅØÔºöÔºö')
            # for item in all_scored_items:
            #     try:
            #         self.logger.info(f"  item: {item['title']}")
            #         self.logger.info(f"  source: {item['source']}")
            #         self.logger.info(f"  journal_name: {item['journal_name']}")
            #         self.logger.info(f"  impact_factor: {item['impact_factor']}")
            #         self.logger.info(f"  source_type: {item['source_type']}")
            #         self.logger.info(f"  authors: {item['authors']}")
            #         self.logger.info(f"  published_date: {item['published_date']}")
            #         self.logger.info(f"  vol_pagination: {item['vol_pagination']}")
            #         self.logger.info(f'------------------------')
            #     except Exception as e:
            #         self.logger.warning(f"ËÆ∞ÂΩïÈ°πÁõÆÊó∂Âá∫Èîô: {str(e)}")
            #         continue

            # # ËÆ∞ÂΩïÊéíÂ∫èÂêéÁöÑÁªìÊûú
            # self.logger.info("\nÊéíÂ∫èÂêéÁöÑÁªìÊûú:")
            # for idx, item in enumerate(all_scored_items):
            #     self.logger.info(f"ÊéíÂêç {idx + 1}:")
            #     self.logger.info(f"  Êù•Ê∫ê: {item['source']}")
            #     self.logger.info(f"  ÊúüÂàäÂêçÁß∞: {item['journal_name']}")
            #     self.logger.info(f"  Êù•Ê∫êÁ±ªÂûã: {item['source_type']}")
            #     self.logger.info(f"  Ê†áÈ¢ò: {item['title']}")
            #     self.logger.info(f"  ÂÜÖÂÆπÈïøÂ∫¶: {len(item['content'])}")
            #     self.logger.info(f"  Áõ∏‰ººÂ∫¶ÂæóÂàÜ: {item['similarity_score']:.4f}")
            #     self.logger.info(f"  ‰∏ä‰∏ãÊñáÊéíÂêçÂæóÂàÜ: {item['context_rank_score']:.4f}")
            #     self.logger.info(f"  Êù•Ê∫êÊùÉÂ®ÅÊÄßÂæóÂàÜ: {item['source_authority_score']:.4f}")
            #     self.logger.info(f"  ÂΩ±ÂìçÂõ†Â≠ê: {item['impact_factor']}")
            #     self.logger.info(f"  Ê†áÂáÜÂåñÂΩ±ÂìçÂõ†Â≠êÂæóÂàÜ: {item['normalized_impact_factor']:.4f}")
            #     self.logger.info(f"  ÊÄªÂàÜ: {item['score']:.4f}")
            #     self.logger.info("-" * 50)
            
            # ËÆæÁΩÆÈòàÂÄºÊàñÂèñÂâçN‰∏™ÁªìÊûú
            threshold = 0.4  # ÈªòËÆ§ÈòàÂÄº
            max_results = 20  # ÊúÄÂ§ßÁªìÊûúÊï∞Èáè
            
            # Â∫îÁî®ÈòàÂÄºÊàñÂèñÂâçN‰∏™
            filtered_items = [item for item in all_scored_items if item['score'] >= threshold]
            if not filtered_items:  # Â¶ÇÊûúÈòàÂÄºËøáÊª§ÂêéÊ≤°ÊúâÁªìÊûúÔºåÂèñÊéíÂêçÂâçNÁöÑÁªìÊûú
                filtered_items = all_scored_items[:max_results]
            elif len(filtered_items) > max_results:  # Â¶ÇÊûúÁªìÊûúËøáÂ§öÔºåÈôêÂà∂Êï∞Èáè
                filtered_items = filtered_items[:max_results]
            
            # ËÆ∞ÂΩïÊúÄÁªàÈÄâÊã©ÁöÑÁªìÊûú
            self.logger.info(f"\nÊúÄÁªàÈÄâÊã©ÁöÑÁªìÊûú (ÈòàÂÄº: {threshold}, ÊúÄÂ§ßÊï∞Èáè: {max_results}):")
            self.logger.info(f"ÈÄâÊã©ÁöÑÈ°πÁõÆÊï∞: {len(filtered_items)}")
            
            # Ê†ºÂºèÂåñËæìÂá∫Ôºå‰øùÊåÅ‰∏éÂéüÂßãÊ†ºÂºèÂÖºÂÆπ
            formatted_context = []
            for item in filtered_items:
                formatted_block = (
                    f"Source: {item['source']}\n"
                    f"Title: {item['title']}\n"
                    f"Content: {item['content']}\n"
                )
                formatted_context.append(formatted_block)

            # Â∞Üfiltered_itemsËΩ¨Êç¢‰∏∫ÂàÜÁ±ªÊ†ºÂºè

            #Âõ∫ÂÆöÂàùÂßãÂÄºÊ†ºÂºèÔºöÂàóË°®
            initial_value = []  #

            classified_items = {key: initial_value.copy() for key in source_type_keys}

            for item in filtered_items:
                source = item['source']
                parsed_block = {
                    "source": source,
                    "JournalName": item['journal_name'],
                    "title": item['title'],
                    "content": item['content'],
                    # Âä†++
                    'impact_factor': item['impact_factor'],
                    'published_date': item.get('published_date', ''),  # Ëé∑ÂèñÂèëÂ∏ÉÊó•ÊúüÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôÈªòËÆ§‰∏∫'',
                    'authors': item.get('authors', ''),
                    'vol_pagination': item.get("vol_pagination")

                }
                
                # if item['source_type'] == "pubmed":
                #     classified_items['pubmed'].append(parsed_block)
                # elif item['source_type'] == "arxiv":
                #     classified_items['arxiv'].append(parsed_block)
                # else:
                #     classified_items['tavily'].append(parsed_block)

                #todo ‰øÆ+
                #Ê£ÄÊü•Êù•Ê∫êÁ±ªÂûãÂπ∂Ê∑ªÂä†Âà∞ÂØπÂ∫îÁöÑÂàÜÁ±ª‰∏≠
                classified_items[item['source_type']].append(parsed_block)
            
            # accumulated_classified_results = accumulated_classified_results
            # # Êõ¥Êñ∞Á¥ØÁßØÁöÑÂàÜÁ±ªÁªìÊûúÔºå‰øùÊåÅÂàÜÁ±ªÁªìÊûÑ
            # self.logger.info("Updating accumulated results")
            # for category in classified_items:
            #     try:
            #         # Ê£ÄÊü•ÊòØÂê¶ÊúâÈáçÂ§çÁöÑsource
            #         existing_sources = {item['source'] for item in self.researcher.accumulated_classified_results[category]}
            #         # Âè™Ê∑ªÂä†Êñ∞ÁöÑsource
            #         new_items = [item for item in classified_items[category] if item['source'] not in existing_sources]
            #         if new_items:
            #             self.logger.info(f"Adding {len(new_items)} new items to {category}")
            #             self.researcher.accumulated_classified_results[category].extend(new_items)
            #     except Exception as e:
            #         self.logger.error(f"Error processing category {category}: {str(e)}")
            #         continue

            #todo ‰øÆ+(ËØï)
            #Êõ¥Êñ∞Á¥ØÁßØÁöÑÂàÜÁ±ªÁªìÊûúÔºå‰øùÊåÅÂàÜÁ±ªÁªìÊûÑ
            self.logger.info("Updating accumulated results")

            for category, context in classified_items.items():
                if not self.researcher.accumulated_classified_results.setdefault(category, None):
                    self.researcher.accumulated_classified_results[category] = []
                le_s = len(self.researcher.accumulated_classified_results[category])
                try:
                    for item_dict in context:
                        if len(self.researcher.accumulated_classified_results[category]) == 0:
                            self.researcher.accumulated_classified_results[category].append(item_dict)
                        else:
                            inf = True
                            for i in range(len(self.researcher.accumulated_classified_results[category])):
                                if self.researcher.accumulated_classified_results[category][i]['source'] == item_dict[
                                    'source']:
                                    self.researcher.accumulated_classified_results[category][i]['content'] += item_dict[
                                        'content']
                                    inf = False
                                    break
                            if inf:
                                self.researcher.accumulated_classified_results[category].append(item_dict)

                    le_e = len(self.researcher.accumulated_classified_results[category])

                    self.logger.info(f"Added {le_e - le_s} new items to {category}")

                except Exception as e:
                    self.logger.error(f"Error processing category {category}: {str(e)}")
                    continue


            # ËÆ∞ÂΩïÂΩìÂâçÁ¥ØÁßØÁöÑÁªìÊûú
            self.logger.info("Current accumulated results:")
            for category in self.researcher.accumulated_classified_results:
                self.logger.info(f"{category}: {len(self.researcher.accumulated_classified_results[category])} items")

            # # ÊµãËØïÁî®Ôºö
            # for category,item_list in self.researcher.accumulated_classified_results.items():
            #     self.logger.info(f"Â±ïÁ§∫ÊêúÁ¥¢ÂºïÊìé{category}: {len(item_list)} itemsÔºåËé∑ÂèñÁöÑÂÜÖÂÆπÔºö"+"\n\n")
            #     for item in item_list:
            #         self.logger.info(f"{item}"+"\n")


            # ËΩ¨Êç¢‰∏∫JSONÂ≠óÁ¨¶‰∏≤
            self.logger.info("Converting to JSON")
            classified_json = json.dumps(self.researcher.accumulated_classified_results, ensure_ascii=False, indent=2)
            self.logger.info(f"ÂàÜÁ±ªÁªìÊûú: {classified_json}") #
            await stream_output(
                    "logs", "subquery_context_window", f"{classified_json}", self.researcher.websocket
                )
            if formatted_context:
                combined_context = " ".join(formatted_context)
                self.logger.info(f"ÊúÄÁªàÁªÑÂêà‰∏ä‰∏ãÊñáÂ§ßÂ∞è: {len(combined_context)}")
                return combined_context
            return []
            
        except Exception as e:
            self.logger.error(f"Error during web search: {e}", exc_info=True)
            import traceback
            self.logger.error(traceback.format_exc())
            return []

    async def _process_sub_query(self, sub_query: str, scraped_data: list = [], query_domains: list = []):
        """Takes in a sub query and scrapes urls based on it and gathers context."""
        if self.json_handler:
            self.json_handler.log_event("sub_query", {
                "query": sub_query,
                "scraped_data_size": len(scraped_data)
            })
        
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_research",
                f"\nüîç Running research for '{sub_query}'...",
                self.researcher.websocket,
            )

        try:
            if not scraped_data:
                scraped_data = await self._scrape_data_by_urls(sub_query, query_domains)
                self.logger.info(f"Scraped data size: {len(scraped_data)}")

            content = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)
            self.logger.info(f"Content found for sub-query: {len(str(content)) if content else 0} chars")
            
            # Ëß£ÊûêÂÜÖÂÆπÂùóÂπ∂Ê∑ªÂä†Ê£ÄÁ¥¢Âô®Á±ªÂûã‰ø°ÊÅØ
            if content:
                content_blocks = re.split(r'\n(?=Source: https?://)', content.strip())
                processed_blocks = []
                
                for block in content_blocks:
                    if not block.strip():
                        continue
                        
                    # Ëß£ÊûêÂùóÂÜÖÂÆπ
                    source_match = re.search(r'^Source: (https?://[^\s]+)', block, re.M)
                    title_match = re.search(r'Title: (.+)', block)
                    content_match = re.search(r'Content: (.+)', block, re.DOTALL)
                    
                    if not all([source_match, title_match, content_match]):
                        continue
                        
                    url = source_match.group(1)
                    title = title_match.group(1).strip()
                    content_text = content_match.group(1).strip()
                    
                    # ‰ªéscraped_data‰∏≠Ëé∑ÂèñÊ£ÄÁ¥¢Âô®Á±ªÂûã
                    retriever_type = "tavily"  # ÈªòËÆ§Á±ªÂûã
                    for item in scraped_data:
                        if item.get('url') == url and 'retriever_type' in item:
                            retriever_type = item['retriever_type']
                            self.logger.info(f"‰ªéscraped_data‰∏≠Ëé∑ÂèñÂà∞Ê£ÄÁ¥¢Âô®Á±ªÂûã: {retriever_type}")
                            break


                    # Âä†++
                    # ‰ªéscraped_data‰∏≠Ëé∑ÂèñÂá∫ÁâàÊó∂Èó¥‰ø°ÊÅØ
                    published_date = ""  # ÈªòËÆ§Âá∫ÁâàÊó∂Èó¥‰∏∫Á©∫
                    for item in scraped_data:
                        if item.get('url') == url and 'published_date' in item:
                            published_date = item['published_date']
                            self.logger.info(f"‰ªéscraped_data‰∏≠Ëé∑ÂèñÂà∞Âá∫ÁâàÊó∂Èó¥: {published_date}")
                            break

                    # ‰ªéscraped_data‰∏≠Ëé∑Âèñ‰ΩúËÄÖ‰ø°ÊÅØ
                    authors = ""  # ÈªòËÆ§‰ΩúËÄÖ‰∏∫Á©∫
                    for item in scraped_data:
                        if item.get('url') == url and 'authors' in item:
                            authors = item['authors']
                            self.logger.info(f"‰ªéscraped_data‰∏≠Ëé∑ÂèñÂà∞‰ΩúËÄÖ‰ø°ÊÅØ: {authors}")
                            break

                    if authors and isinstance(authors, list):  # Â¶ÇÊûú‰ΩúËÄÖ‰ø°ÊÅØÂ≠òÂú®,
                        author_names = ""
                        # ÊèêÂèñ‰ΩúËÄÖÁöÑÂêçÂ≠ó
                        for idx in range(len(authors)):
                            if idx > authors_t - 1:
                                break
                            author_names += authors[idx] + ", "
                        authors = author_names[:-2] + ", et al."
                        self.logger.info(f"ÊúÄÂêéÊèêÂèñÂà∞ÁöÑ‰ΩúËÄÖ‰ø°ÊÅØ: {authors}")

                    # ‰ªéscraped_data‰∏≠Ëé∑ÂèñÂç∑Âè∑‰ø°ÊÅØ
                    vol = ""  # ÈªòËÆ§Âç∑Âè∑‰∏∫Á©∫
                    for item in scraped_data:
                        if item.get('url') == url and 'vol' in item:
                            vol = item['vol']
                            self.logger.info(f"‰ªéscraped_data‰∏≠Ëé∑ÂèñÂà∞Âç∑Âè∑‰ø°ÊÅØ: {vol}")
                            break

                    # ‰ªéscraped_data‰∏≠Ëé∑ÂèñÈ°µÁ†Å‰ø°ÊÅØ
                    pagination = ""  # ÈªòËÆ§È°µÁ†Å‰∏∫Á©∫
                    for item in scraped_data:
                        if item.get('url') == url and 'pagination' in item:
                            pagination = item['pagination']
                            self.logger.info(f"‰ªéscraped_data‰∏≠Ëé∑ÂèñÂà∞È°µÁ†Å‰ø°ÊÅØ: {pagination}")
                            break

                    # ‰øÆ++
                    # ÊûÑÂª∫Êñ∞ÁöÑÂÜÖÂÆπÂùóÔºåÂåÖÂê´Ê£ÄÁ¥¢Âô®Á±ªÂûã‰ø°ÊÅØ
                    processed_block = (
                        f"Source: {url}\n"
                        f"Title: {title}\n"
                        f"Content: {content_text}\n"
                        f"RetrieverType: {retriever_type}\n"
                        f"Published Date: {published_date}\n"
                        f"Volume: {vol}\n"
                        f"Pagination: {pagination}\n"
                        f"Authors: {authors}\n"

                    )
                    processed_blocks.append(processed_block)
                
                # ÈáçÊñ∞ÁªÑÂêàÂ§ÑÁêÜÂêéÁöÑÂÜÖÂÆπ
                content = "\n".join(processed_blocks)

            if content and self.researcher.verbose:
                print(f"Content found 12345")
            elif self.researcher.verbose:
                await stream_output(
                    "logs",
                    "subquery_context_not_found",
                    f"ü§∑ No content found for '{sub_query}'...",
                    self.researcher.websocket,
                )
            if content:
                if self.json_handler:
                    self.json_handler.log_event("content_found", {
                        "sub_query": sub_query,
                        "content_size": len(content)
                    })

            return content
        except Exception as e:
            self.logger.error(f"Error processing sub-query {sub_query}: {e}", exc_info=True)
            return ""


    async def _process_sub_query_with_vectorstore(self, sub_query: str, filter: Optional[dict] = None):
        """Takes in a sub query and gathers context from the user provided vector store

        Args:
            sub_query (str): The sub-query generated from the original query

        Returns:
            str: The context gathered from search
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_with_vectorstore_research",
                f"\nüîç Running research for '{sub_query}'...",
                self.researcher.websocket,
            )

        content = await self.researcher.context_manager.get_similar_content_by_query_with_vectorstore(sub_query, filter)

        if content and self.researcher.verbose:
            await stream_output(
                "logs", "subquery_context_window", f"üìÉ {content}", self.researcher.websocket
            )
        elif self.researcher.verbose:
            await stream_output(
                "logs",
                "subquery_context_not_found",
                f"ü§∑ No content found for '{sub_query}'...",
                self.researcher.websocket,
            )
        return content

    async def classify_content(self, content: str) -> Dict[str, List[Dict]]:
        """
        ÂºÇÊ≠•ÂàÜÁ±ªÂÜÖÂÆπÂà∞‰∏çÂêåÁ±ªÂà´Ôºàarxiv/tavilyÔºâ
    
         Args:
            content: ÂåÖÂê´Â§ö‰∏™ÂÜÖÂÆπÂùóÁöÑÂéüÂßãÊñáÊú¨
        
        Returns:
        ÂàÜÁ±ªÂêéÁöÑÂ≠óÂÖ∏ÁªìÊûÑ {
            "arxiv": [ÂåÖÂê´arxivÁöÑÂùó],
            "tavily": [ÂÖ∂‰ªñÂùó]
        }
        """
        # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÂàÜÂâ≤ÂÜÖÂÆπÂùó
        blocks = re.split(r'\n(?=Source: https?://)', content.strip())
        
        classified = {"arxiv": [], "pubmed": [], "tavily": []}  # ÂàùÂßãÂåñÊâÄÊúâÂèØËÉΩÁöÑÁ±ªÂà´
        
        for block in blocks:
            if not block.strip():
                continue
                
            # Ëß£ÊûêÂùóÂÜÖÂÆπ
            source_match = re.search(r'^Source: (https?://[^\s]+)', block, re.M)
            title_match = re.search(r'Title: (.+)', block)
            content_match = re.search(r'Content: (.+)', block, re.DOTALL)
            
            if not all([source_match, title_match, content_match]):
                continue
                
            parsed_block = {
                "source": source_match.group(1), 
                "title": title_match.group(1).strip().replace('\n', ''),
                "content": content_match.group(1).strip().replace('\n', ' ')
            }

            # ÂàÜÁ±ªÈÄªËæë
            if 'arxiv' in parsed_block['source'].lower():
                classified['arxiv'].append(parsed_block)
            elif 'ncbi' in parsed_block['source'].lower():
                classified['pubmed'].append(parsed_block)
            else:
                classified['tavily'].append(parsed_block)

        # Â∞ÜÊ≤°ÊúâÂÜÖÂÆπÁöÑÂàÜÁ±ªËøõË°åÂâîÈô§Ôºå‰∏çÂú®ËæìÂá∫Â±ïÁ§∫
        classified = {key: value for key, value in classified.items() if value}
        # ‰ΩøÁî® json.dumps Â∞ÜÂ≠óÂÖ∏ËΩ¨Êç¢‰∏∫ JSON Â≠óÁ¨¶‰∏≤ÔºåÁ°Æ‰øù‰ΩøÁî®ÂèåÂºïÂè∑
        classified_json = json.dumps(classified, ensure_ascii=False, indent=2)

        return classified_json

    async def _get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """

        new_urls = []
        for url in url_set_input:
            if url not in self.researcher.visited_urls:
                self.researcher.visited_urls.add(url)
                new_urls.append(url)
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "added_source_url",
                        f"‚úÖ Added source url to research: {url}\n",
                        self.researcher.websocket,
                        True,
                        url,
                    )

        return new_urls

    async def _search_relevant_source_urls(self, query, query_domains: list = []):
        new_search_urls = []
        search_results_with_type = []  # Â≠òÂÇ®Â∏¶Ê£ÄÁ¥¢Âô®Á±ªÂûãÁöÑÁªìÊûú

        # ÈÅçÂéÜÊâÄÊúâÊ£ÄÁ¥¢Âô®
        for retriever_class in self.researcher.retrievers:
            # Ëé∑ÂèñÂΩìÂâçÊ£ÄÁ¥¢Âô®Á±ªÂûã
            current_retriever_type = retriever_class.__name__.lower() 
            # Êò†Â∞ÑÊ£ÄÁ¥¢Âô®Á±ªÂûã
            retriever_type_mapping = {
                'tavilysearch': 'tavily',
                'arxivsearch': 'arxiv',
                'pubmeddiansearch': 'pubmed'  # ‰ΩøÁî®Â∞èÂÜôÂΩ¢Âºè
            }
            current_retriever_type = retriever_type_mapping.get(current_retriever_type, 'tavily')
            self.logger.info(f"Current retriever type: {current_retriever_type} (original: {retriever_class.__name__})")
            
            # Ê†πÊçÆÊ£ÄÁ¥¢Âô®Á±ªÂûãÂ§ÑÁêÜÊü•ËØ¢
            processed_query = query
            if current_retriever_type == "pubmed":
                # ÂØπ PubMed Ê£ÄÁ¥¢Âô®ËøõË°åÁâπÊÆäÂ§ÑÁêÜ
                self.logger.info("Processing query for PubMed retriever")
                # ‰ΩøÁî® generate_pubmed_sub_queries ÁîüÊàêËßÑËåÉÂåñÁöÑÊü•ËØ¢
                sub_queries = await generate_pubmed_sub_queries(
                    query=query,
                    cfg=self.researcher.cfg,
                    cost_callback=self.researcher.add_costs
                )
                # ‰ΩøÁî®Á¨¨‰∏Ä‰∏™Â≠êÊü•ËØ¢‰Ωú‰∏∫‰∏ªË¶ÅÊü•ËØ¢
                if sub_queries:
                    processed_query = sub_queries[0]
                    self.logger.info(f"Processed PubMed query: {processed_query}")
                else:
                    self.logger.warning("No PubMed sub-queries generated, skipping PubMed search")
                    continue  # Â¶ÇÊûúÊ≤°ÊúâÁîüÊàêÂ≠êÊü•ËØ¢ÔºåË∑≥ËøáÂΩìÂâçÊ£ÄÁ¥¢Âô®
            
            # ÂÆû‰æãÂåñÂΩìÂâçÊ£ÄÁ¥¢Âô®
            retriever = retriever_class(processed_query, query_domains=query_domains)
            self.logger.info(f"*****use retriever: {current_retriever_type} search query: {processed_query}")
            # ÊâßË°åÊêúÁ¥¢
            search_results = await asyncio.to_thread(
                retriever.search, max_results=self.researcher.cfg.max_search_results_per_query
            )
            self.logger.info(f"*****use retriever 2: {current_retriever_type}")
            ceshi_c = 0
            # ‰∏∫ÊêúÁ¥¢ÁªìÊûúÊ∑ªÂä†Ê£ÄÁ¥¢Âô®Á±ªÂûãÊ†áËØÜ
            for result in search_results:
                result['retriever_type'] = current_retriever_type  # Ê∑ªÂä†Ê£ÄÁ¥¢Âô®Á±ªÂûãÂ≠óÊÆµ
                search_results_with_type.append(result)  # Â∞ÜÁªìÊûúÊ∑ªÂä†Âà∞Â∏¶Ê£ÄÁ¥¢Âô®Á±ªÂûãÁöÑÁªìÊûúÂàóË°®‰∏≠
                ceshi_c += 1
                self.logger.info(f"ÊµãËØïÊ£ÄÁ¥¢Âô®Ôºö<{ceshi_c}>{current_retriever_type} \n ÊêúÁ¥¢ÁªìÊûúÔºö{result['href']} ")
                # self.logger.info(f"ÊµãËØïpubmedÊêúÁ¥¢ÁöÑÂ≠óÊÆµÊòØÂê¶Â≠òÂú®Ôºö0Ôºö{result.get('href')}")
                # self.logger.info(f"ÊµãËØïpubmedÊêúÁ¥¢ÁöÑÂ≠óÊÆµÊòØÂê¶Â≠òÂú®Ôºö1Ôºö{result.get('published')}")
                # self.logger.info(f"ÊµãËØïpubmedÊêúÁ¥¢ÁöÑÂ≠óÊÆµÊòØÂê¶Â≠òÂú®Ôºö2Ôºö{result.get('pagination')}")
                # self.logger.info(f"ÊµãËØïpubmedÊêúÁ¥¢ÁöÑÂ≠óÊÆµÊòØÂê¶Â≠òÂú®Ôºö3Ôºö{result.get('authors')}")
                # self.logger.info(f"ÊµãËØïpubmedÊêúÁ¥¢ÁöÑÂ≠óÊÆµÊòØÂê¶Â≠òÂú®Ôºö4Ôºö{result.get('vol')}")
                # self.logger.info(f"ÊµãËØïpubmedÊêúÁ¥¢ÁöÑÂ≠óÊÆµÊòØÂê¶Â≠òÂú®Ôºö5Ôºö{result.get('title')}")

            # Êî∂ÈõÜURL
            search_urls = [url.get("href") for url in search_results]
            new_search_urls.extend(search_urls)

        # Ëé∑ÂèñÊñ∞ÁöÑURLÂπ∂ÈöèÊú∫Êâì‰π±
        new_search_urls = await self._get_new_urls(new_search_urls)
        random.shuffle(new_search_urls)

        # ËøîÂõûURLÂàóË°®ÂíåÂ∏¶Ê£ÄÁ¥¢Âô®Á±ªÂûãÁöÑÁªìÊûú
        return new_search_urls, search_results_with_type

    async def _scrape_data_by_urls(self, sub_query, query_domains: list = []):
        """
        Runs a sub-query across multiple retrievers and scrapes the resulting URLs.

        Args:
            sub_query (str): The sub-query to search for.

        Returns:
            list: A list of scraped content results.
        """
        # Ëé∑ÂèñURLÂàóË°®ÂíåÂ∏¶Ê£ÄÁ¥¢Âô®Á±ªÂûãÁöÑÁªìÊûú
        new_search_urls, search_results_with_type = await self._search_relevant_source_urls(sub_query, query_domains)

        # Log the research process if verbose mode is on
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "researching",
                f"ü§î Researching for relevant information across multiple sources...\n",
                self.researcher.websocket,
            )

        # Scrape the new URLs
        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)
        
        # ËÆ∞ÂΩïÊäìÂèñÁªìÊûú
        self.logger.info(f"ÊäìÂèñÁªìÊûúÊï∞Èáè: {len(scraped_content)}")
        
        # ÂàõÂª∫URLÂà∞Ê£ÄÁ¥¢Âô®Á±ªÂûãÁöÑÊò†Â∞Ñ
        url_to_type = {}

        # Âä†++
        # ÂàùÂßãÂåñÂÖ∂‰ªñÂ≠óÊÆµÁöÑÊò†Â∞Ñ
        url_to_published_date = {}
        url_to_authors = {}
        url_to_vol = {}
        url_to_pagination = {}

        for result in search_results_with_type:
            if 'href' in result and 'retriever_type' in result:
                normalized_url = self._normalize_url(result['href'])
                url_to_type[normalized_url] = result['retriever_type']
                self.logger.info(f"Ê∑ªÂä†Ê£ÄÁ¥¢Âô®<-URLÊò†Â∞Ñ:{result['retriever_type']} <-- {normalized_url}")

                ## Âä†++‰øÆ++
                # if result['retriever_type'] == 'pubmed':  # Â¶ÇÊûúÊ£ÄÁ¥¢Âô®Á±ªÂûãÊòØpubmed
                self.logger.info(f"pubmedÊ£ÄÁ¥¢Âô®Á±ªÂûã, ÂºÄÂßãÂ§ÑÁêÜpubmedÂ≠óÊÆµ")
                if 'published' in result:  # Â¶ÇÊûúÁªìÊûú‰∏≠ÂåÖÂê´ÂèëÂ∏ÉÊó•ÊúüÂ≠óÊÆµ
                    url_to_published_date[normalized_url] = result['published']  # Ê∑ªÂä†Âà∞Êò†Â∞Ñ‰∏≠
                    self.logger.info(f"Ê∑ªÂä†ÂèëÂ∏ÉÊó•ÊúüÊò†Â∞Ñ: {normalized_url} -> {result['published']}")
                if 'authors' in result:  # Â¶ÇÊûúÁªìÊûú‰∏≠ÂåÖÂê´‰ΩúËÄÖÂ≠óÊÆµ
                    url_to_authors[normalized_url] = result['authors']  # Ê∑ªÂä†Âà∞Êò†Â∞Ñ‰∏≠
                    self.logger.info(f"Ê∑ªÂä†‰ΩúËÄÖÊò†Â∞Ñ: {normalized_url} -> {result['authors']}")
                if 'vol' in result:  # Â¶ÇÊûúÁªìÊûú‰∏≠ÂåÖÂê´Âç∑Âè∑Â≠óÊÆµ
                    url_to_vol[normalized_url] = result['vol']  # Ê∑ªÂä†Âà∞Êò†Â∞Ñ‰∏≠
                    self.logger.info(f"Ê∑ªÂä†Âç∑Âè∑Êò†Â∞Ñ: {normalized_url} -> {result['vol']}")
                if 'pagination' in result:  # Â¶ÇÊûúÁªìÊûú‰∏≠ÂåÖÂê´ÂàÜÈ°µ‰ø°ÊÅØÂ≠óÊÆµ
                    url_to_pagination[normalized_url] = result['pagination']  # Ê∑ªÂä†Âà∞Êò†Â∞Ñ‰∏≠
                    self.logger.info(f"Ê∑ªÂä†ÂàÜÈ°µ‰ø°ÊÅØÊò†Â∞Ñ: {normalized_url} -> {result['pagination']}")


        self.logger.info(f"ÂèëÂ∏ÉÊó∂Èó¥ÁöÑÊò†Â∞ÑÂ≠óÂÖ∏ÁöÑÈïøÂ∫¶ÊòØÔºö{len(url_to_published_date)}")

        # ‰∏∫ÊØè‰∏™ÂÜÖÂÆπÊ∑ªÂä†Ê£ÄÁ¥¢Âô®Á±ªÂûã‰ø°ÊÅØ
        for content in scraped_content:
            if 'url' in content:
                content_url = content['url']
                normalized_content_url = self._normalize_url(content_url)
                self.logger.info(f"Â§ÑÁêÜURL: {content_url}")
                self.logger.info(f"Ê†áÂáÜÂåñÂêéÁöÑURL: {normalized_content_url}")
                
                # È¶ñÂÖàÂ∞ùËØï‰ªéÊò†Â∞Ñ‰∏≠Ëé∑ÂèñÁ±ªÂûã
                if normalized_content_url in url_to_type:
                    content['retriever_type'] = url_to_type[normalized_content_url]
                    self.logger.info(f"‰ªéURLÊò†Â∞Ñ‰∏≠Ëé∑ÂèñÂà∞Ê£ÄÁ¥¢Âô®Á±ªÂûã: {content['retriever_type']}")
                else:
                    # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÂåπÈÖçÁöÑÊ£ÄÁ¥¢Âô®Á±ªÂûãÔºå‰ΩøÁî®URLÁâπÂæÅÂà§Êñ≠
                    if 'arxiv' in normalized_content_url:
                        content['retriever_type'] = 'arxiv'
                        self.logger.info(f"Ê†πÊçÆURLÁâπÂæÅÂà§Êñ≠‰∏∫arxiv")
                    elif 'ncbi' in normalized_content_url or 'pubmed' in normalized_content_url:
                        content['retriever_type'] = 'pubmed'
                        self.logger.info(f"Ê†πÊçÆURLÁâπÂæÅÂà§Êñ≠‰∏∫pubmed")
                    else:
                        content['retriever_type'] = 'tavily'
                        self.logger.info(f"Ê†πÊçÆURLÁâπÂæÅÂà§Êñ≠‰∏∫tavily")

                ## ‰øÆ++
                # ‰∏∫ÊØè‰∏™ÂÜÖÂÆπÊ∑ªÂä†:1. Âá∫ÁâàÊó∂Èó¥ published_date 2. ÊúüÂàäÂç∑Âè∑ vol  3. ‰ΩúËÄÖ‰ø°ÊÅØ authors 4. ÂàÜÈ°µ‰ø°ÊÅØ pagination
                if normalized_content_url in url_to_published_date:  # Â¶ÇÊûúURLÂú®ÂèëÂ∏ÉÊó•ÊúüÊò†Â∞Ñ‰∏≠
                    content['published_date'] = url_to_published_date[normalized_content_url]  # Ê∑ªÂä†ÂèëÂ∏ÉÊó•ÊúüÂ≠óÊÆµ
                    self.logger.info(f"Ê∑ªÂä†ÂèëÂ∏ÉÊó•Êúü: {content['published_date']}")
                if normalized_content_url in url_to_authors:  # Â¶ÇÊûúURLÂú®‰ΩúËÄÖÊò†Â∞Ñ‰∏≠
                    content['authors'] = url_to_authors[normalized_content_url]  # Ê∑ªÂä†‰ΩúËÄÖÂ≠óÊÆµ
                    self.logger.info(f"Ê∑ªÂä†‰ΩúËÄÖ: {content['authors']}")
                if normalized_content_url in url_to_vol:  # Â¶ÇÊûúURLÂú®Âç∑Âè∑Êò†Â∞Ñ‰∏≠
                    content['vol'] = url_to_vol[normalized_content_url]  # Ê∑ªÂä†Âç∑Âè∑Â≠óÊÆµ
                    self.logger.info(f"Ê∑ªÂä†Âç∑Âè∑: {content['vol']}")
                if normalized_content_url in url_to_pagination:  # Â¶ÇÊûúURLÂú®ÂàÜÈ°µ‰ø°ÊÅØÊò†Â∞Ñ‰∏≠
                    content['pagination'] = url_to_pagination[normalized_content_url]  # Ê∑ªÂä†ÂàÜÈ°µ‰ø°ÊÅØÂ≠óÊÆµ
                    self.logger.info(f"Ê∑ªÂä†ÂàÜÈ°µ‰ø°ÊÅØ: {content['pagination']}")


        if self.researcher.vector_store:
            self.researcher.vector_store.load(scraped_content)

        return scraped_content

    def _normalize_url(self, url):
        """Ê†áÂáÜÂåñURL‰ª•‰æøËøõË°åÂåπÈÖç"""
        if not url:
            return ""
        # ÁßªÈô§URL‰∏≠ÁöÑÂçèËÆÆÂâçÁºÄ
        url = re.sub(r'^https?://', '', url)
        # ÁßªÈô§Â∞æÈÉ®ÊñúÊù†
        url = url.rstrip('/')
        # ÁßªÈô§URLÂèÇÊï∞
        url = url.split('?')[0]
        # ÁßªÈô§ÈîöÁÇπ
        url = url.split('#')[0]
        return url.lower()

    async def _normalize_journal_name(self, name):
        """Ê†áÂáÜÂåñÊúüÂàäÂêçÁß∞‰ª•ÊèêÈ´òÂåπÈÖçÊàêÂäüÁéá"""
        if not name:
            return name
            
        # Ëß£Á†ÅHTMLÂÆû‰Ωì
        normalized = html.unescape(name)
        
        # Ê†áÂáÜÂåñÁ©∫Ê†º
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # Ê†áÂáÜÂåñËøûÂ≠óÁ¨¶ÂíåÂ∏∏ËßÅÊõøÊç¢
        normalized = normalized.replace(' - ', '-')
        normalized = normalized.replace(' And ', ' & ')
        normalized = normalized.replace(' and ', ' & ')
        normalized = normalized.replace('&amp;', '&')
        normalized = normalized.replace('&AMP;', '&')
        
        # Â§ÑÁêÜÂ∏∏ËßÅÁöÑÊúüÂàäÂêçÁß∞Âèò‰Ωì
        replacements = {
            'J ': 'Journal ',
            'Intl': 'International',
            'Int ': 'International ',
            'Int.': 'International',
            'Rev ': 'Review ',
            'Rev.': 'Review',
            'Sci ': 'Science ',
            'Sci.': 'Science',
            'Adv ': 'Advances ',
            'Adv.': 'Advances',
            'Res ': 'Research ',
            'Res.': 'Research',
            'Chem ': 'Chemistry ',
            'Chem.': 'Chemistry',
            'Biol ': 'Biology ',
            'Biol.': 'Biology',
            'Med ': 'Medicine ',
            'Med.': 'Medicine',
        }
        
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        return normalized

    async def _extract_journal_info_from_url(self, url):
        """‰ªéURL‰∏≠ÊèêÂèñÊúüÂàä‰ø°ÊÅØÔºå‰ºòÂÖàÈÄöËøáDOIËØÜÂà´"""
        journal_info = {"journal_name": None}
        
        try:
            # È¶ñÂÖàÂ∞ùËØïÁõ¥Êé•‰ªéURLÊèêÂèñDOI
            doi_match = re.search(r'(10\.\d{4,}[\/.][\w\.\-\/]+)', url)
            
            # Â¶ÇÊûúURL‰∏≠Ê≤°ÊúâDOIÔºåÂ∞ùËØïËÆøÈóÆÈ°µÈù¢Ëé∑ÂèñDOI
            if not doi_match:
                content = await self._fetch_url_content(url)
                if content:
                    # Â∞ùËØï‰ªéHTML‰∏≠ÊèêÂèñDOI
                    soup = BeautifulSoup(content, "html.parser")
                    
                    # Ê£ÄÊü•ÂêÑÁßçÂèØËÉΩÂåÖÂê´DOIÁöÑÂÖÉÊï∞ÊçÆÊ†áÁ≠æ
                    meta_doi = soup.find("meta", {"name": "citation_doi"}) or \
                            soup.find("meta", {"name": "dc.identifier"}) or \
                            soup.find("meta", {"name": "DC.Identifier"}) or \
                            soup.find("meta", {"scheme": "doi"})
                    
                    if meta_doi and meta_doi.get("content"):
                        doi = meta_doi.get("content")
                        if doi.startswith("doi:"):
                            doi = doi[4:]
                        doi_match = re.match(r'(10\.\d{4,}[\/.][\w\.\-\/]+)', doi)
                    else:
                        # Â∞ùËØï‰ªéÊ≠£Êñá‰∏≠Êü•ÊâæDOI
                        doi_regex = r'(?:doi|DOI):\s*(10\.\d{4,}[\/.][\w\.\-\/]+)'
                        content_match = re.search(doi_regex, content)
                        if content_match:
                            doi_match = re.match(r'(10\.\d{4,}[\/.][\w\.\-\/]+)', content_match.group(1))
            
            # Â¶ÇÊûúÊâæÂà∞DOIÔºåÈÄöËøáCrossRef APIËé∑ÂèñÊúüÂàä‰ø°ÊÅØ
            if doi_match:
                doi = doi_match.group(1)
                journal_info["doi"] = doi
                self.logger.info(f"Found DOI: {doi}")
                
                # Ë∞ÉÁî®CrossRef API
                try:
                    crossref_api_url = f"https://api.crossref.org/works/{doi}"
                    response = await self._fetch_url_content(crossref_api_url)
                    
                    if response:
                        data = json.loads(response)
                        if "message" in data:
                            message = data["message"]
                            
                            # ÊèêÂèñÊúüÂàäÂêçÁß∞
                            if "container-title" in message and message["container-title"]:
                                journal_info["journal_name"] = message["container-title"][0]
                                self.logger.info(f"Found journal name from CrossRef: {journal_info['journal_name']}")
                            
                            # ÊèêÂèñISSN (ÂèØÁî®‰∫éËøõ‰∏ÄÊ≠•ÂåπÈÖçÊúüÂàäÂΩ±ÂìçÂõ†Â≠ê)
                            if "ISSN" in message and message["ISSN"]:
                                journal_info["issn"] = message["ISSN"][0]
                                
                            # ÊèêÂèñÂá∫ÁâàÂïÜ‰ø°ÊÅØ
                            if "publisher" in message:
                                journal_info["publisher"] = message["publisher"]
                                
                            return journal_info
                except Exception as e:
                    self.logger.warning(f"Error fetching CrossRef metadata: {e}")
            
            # Â¶ÇÊûúÈÄöËøáDOIÊó†Ê≥ïËé∑ÂèñÔºåÂõûÈÄÄÂà∞Âü∫‰∫éURLÁöÑÊñπÊ≥ï
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            path = parsed_url.path
            
            # Â§ÑÁêÜÂ∏∏ËßÅÂ≠¶ÊúØÁΩëÁ´ôÁöÑURLÊ®°Âºè
            # arXiv
            if "arxiv.org" in domain:
                journal_info["journal_name"] = "arXiv"
                return journal_info
                
            # PubMed/PMC
            if "pubmed.ncbi.nlm.nih.gov" in domain or "pmc.ncbi.nlm.nih.gov" in domain:
                if "pubmed.ncbi.nlm.nih.gov" in domain:
                    pmid_match = re.search(r'pubmed\.ncbi\.nlm\.nih\.gov\/(\d+)', url)
                    if pmid_match:
                        pmid = pmid_match.group(1)
                        try:
                            api_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id={pmid}&retmode=json"
                            response = await self._fetch_url_content(api_url)
                            if response:
                                data = json.loads(response)
                                if 'result' in data and pmid in data['result']:
                                    result = data['result'][pmid]
                                    if 'fulljournalname' in result:
                                        journal_info["journal_name"] = result['fulljournalname']
                                    elif 'source' in result:
                                        journal_info["journal_name"] = result['source']
                        except Exception as e:
                            self.logger.warning(f"Error fetching PubMed metadata: {e}")
                return journal_info
            
            # Nature
            if "nature.com" in domain:
                if "nature.com/articles/s41586" in url:
                    journal_info["journal_name"] = "Nature"
                elif "nature.com/articles/s41467" in url:
                    journal_info["journal_name"] = "Nature Communications"
                elif re.search(r'nature\.com\/([^\/]+)\/journal', url):
                    journal_match = re.search(r'nature\.com\/([^\/]+)\/journal', url)
                    if journal_match:
                        journal_name = journal_match.group(1).replace('-', ' ').title()
                        journal_info["journal_name"] = f"Nature {journal_name}"
                else:
                    # Â∞ùËØï‰ªésÂºÄÂ§¥ÁöÑDOIÈ£éÊ†ºË∑ØÂæÑ‰∏≠ÊèêÂèñÊúüÂàäÊ†áËØÜÁ¨¶
                    s_match = re.search(r'nature\.com\/articles\/(s\d+)', url)
                    if s_match:
                        # NatureÊúüÂàäDOIÂâçÁºÄÊò†Â∞ÑË°®
                        nature_prefixes = {
                            's41586': 'Nature',
                            's41467': 'Nature Communications',
                            's41598': 'Scientific Reports',
                            's41587': 'Nature Biotechnology',
                            's41591': 'Nature Medicine',
                            's41593': 'Nature Neuroscience',
                            's41594': 'Nature Structural & Molecular Biology',
                            's41561': 'Nature Geoscience',
                            's41563': 'Nature Materials',
                            's41565': 'Nature Nanotechnology',
                            's41566': 'Nature Photonics',
                            's41567': 'Nature Physics',
                            's41577': 'Nature Reviews Immunology',
                            's41573': 'Nature Reviews Drug Discovery',
                            's41574': 'Nature Reviews Endocrinology',
                            's41575': 'Nature Reviews Gastroenterology & Hepatology',
                            's41576': 'Nature Reviews Genetics',
                            's41577': 'Nature Reviews Immunology',
                            's41578': 'Nature Reviews Materials',
                            's41579': 'Nature Reviews Microbiology',
                            's41580': 'Nature Reviews Molecular Cell Biology',
                            's41581': 'Nature Reviews Nephrology',
                            's41582': 'Nature Reviews Neurology',
                            's41584': 'Nature Reviews Rheumatology',
                            's41571': 'Nature Reviews Clinical Oncology',
                            's42255': 'Nature Metabolism',
                            's42256': 'Nature Machine Intelligence',
                            's41567': 'Nature Physics',
                            's41557': 'Nature Chemistry',
                            's41558': 'Nature Climate Change',
                            's41559': 'Nature Ecology & Evolution',
                            's41560': 'Nature Energy',
                            's41564': 'Nature Microbiology',
                            's41570': 'Nature Reviews Chemistry',
                            's41589': 'Nature Chemical Biology',
                            's41592': 'Nature Methods',
                            's41596': 'Nature Protocols',
                            's41597': 'Scientific Data',
                            's41598': 'Scientific Reports',
                            's41699': 'Communications Biology',
                            's41746': 'npj Digital Medicine',
                            's42003': 'Communications Biology',
                        }
                        
                        s_id = s_match.group(1)
                        for prefix, journal in nature_prefixes.items():
                            if s_id.startswith(prefix):
                                journal_info["journal_name"] = journal
                                break
                
                return journal_info
            
            # BioMedCentral
            if "biomedcentral.com" in domain:
                journal_subdomain = domain.split(".biomedcentral.com")[0]
                bmc_journals = {
                    "ann-clinmicrob": "Annals of Clinical Microbiology and Antimicrobials",
                    "bmcgenomics": "BMC Genomics",
                    "bmcinfectdis": "BMC Infectious Diseases",
                    "bmcmicrobiol": "BMC Microbiology",
                    "genomemedicine": "Genome Medicine",
                    "translationalmedicine": "Journal of Translational Medicine",
                    "bmcbiol": "BMC Biology",
                    "bmccancer": "BMC Cancer",
                    "bmcneurosci": "BMC Neuroscience",
                    "bmcpsychiatry": "BMC Psychiatry",
                    "jnanobiotechnology": "Journal of Nanobiotechnology",
                    "mbio": "mBio",
                    "microbiome": "Microbiome",
                    "parasitesandvectors": "Parasites & Vectors",
                    "retrovirology": "Retrovirology",
                    "virologyj": "Virology Journal",
                }
                
                if journal_subdomain in bmc_journals:
                    journal_info["journal_name"] = bmc_journals[journal_subdomain]
                
                return journal_info
            
            # Lancet
            if "thelancet.com" in domain:
                if "thelancet.com/journals/lancet" in url:
                    journal_info["journal_name"] = "The Lancet"
                else:
                    lancet_match = re.search(r'thelancet\.com\/journals\/([^\/]+)', url)
                    if lancet_match:
                        journal_code = lancet_match.group(1)
                        lancet_journals = {
                            "laninf": "The Lancet Infectious Diseases",
                            "lanmic": "The Lancet Microbe",
                            "lanepe": "The Lancet Regional Health - Europe",
                            "lanpsy": "The Lancet Psychiatry",
                            "landia": "The Lancet Diabetes & Endocrinology",
                            "langas": "The Lancet Gastroenterology & Hepatology",
                            "lanhae": "The Lancet Haematology",
                            "lanhiv": "The Lancet HIV",
                            "laneur": "The Lancet Neurology",
                            "lanplh": "The Lancet Planetary Health",
                            "lanpub": "The Lancet Public Health",
                            "lanres": "The Lancet Respiratory Medicine",
                            "lanrhe": "The Lancet Rheumatology",
                            "lanonc": "The Lancet Oncology",
                            "lancet": "The Lancet",
                            "lanchi": "The Lancet Child & Adolescent Health",
                            "landig": "The Lancet Digital Health",
                            "eclinm": "EClinicalMedicine",
                        }
                        if journal_code in lancet_journals:
                            journal_info["journal_name"] = lancet_journals[journal_code]
                
                return journal_info
            
            # MDPI
            if "mdpi.com" in domain:
                mdpi_match = re.search(r'mdpi\.com\/journal\/([^\/]+)', url)
                if mdpi_match:
                    journal_name = mdpi_match.group(1).replace('-', ' ').title()
                    journal_info["journal_name"] = journal_name
                else:
                    # Â∞ùËØï‰ªéURLÊèêÂèñISSN
                    issn_match = re.search(r'mdpi\.com\/(\d{4}-\d{3}[\dX])', url)
                    if issn_match:
                        journal_info["issn"] = issn_match.group(1)
                
                return journal_info
            
            # Frontiers
            if "frontiersin.org" in domain:
                frontiers_match = re.search(r'frontiersin\.org\/journals\/([^\/]+)', url)
                if frontiers_match:
                    journal_slug = frontiers_match.group(1).replace('-', ' ')
                    journal_info["journal_name"] = f"Frontiers in {journal_slug.title()}"
                
                return journal_info
            
            # RSC (Royal Society of Chemistry)
            if "rsc.org" in domain or "pubs.rsc.org" in domain:
                rsc_match = re.search(r'\/([^\/]+)\/article', url)
                if rsc_match:
                    journal_code = rsc_match.group(1)
                    rsc_journals = {
                        "c0": "Chemical Communications",
                        "cc": "Chemical Communications",
                        "cs": "Chemical Science",
                        "dt": "Dalton Transactions",
                        "gc": "Green Chemistry",
                        "cp": "Chemical Physics",
                        "sc": "Sustainable Chemistry",
                        "an": "Analyst",
                        "ce": "Chemical Education",
                        "cy": "Catalysis Science & Technology",
                        "ee": "Energy & Environmental Science",
                        "en": "Environmental Science",
                        "fd": "Faraday Discussions",
                        "lc": "Lab on a Chip",
                        "nr": "Nanoscale",
                        "ob": "Organic & Biomolecular Chemistry",
                        "py": "Polymer Chemistry",
                        "ra": "RSC Advances",
                        "sm": "Soft Matter",
                    }
                    if journal_code in rsc_journals:
                        journal_info["journal_name"] = rsc_journals[journal_code]
                
                return journal_info
            
            # Â¶ÇÊûú‰ªçÊó†Ê≥ïÁ°ÆÂÆöÊúüÂàäÔºåÂ∞ùËØï‰ªéÂÜÖÂÆπ‰∏≠ÊèêÂèñ
            if not journal_info["journal_name"] and not content:
                content = await self._fetch_url_content(url)
                
            if content and not journal_info["journal_name"]:
                soup = BeautifulSoup(content, "html.parser")
                
                # Â∞ùËØï‰ªéÂÖÉÊï∞ÊçÆ‰∏≠ÊèêÂèñÊúüÂàäÂêç
                journal_meta = soup.find("meta", {"name": "citation_journal_title"}) or \
                            soup.find("meta", {"name": "prism.publicationName"}) or \
                            soup.find("meta", {"name": "dc.source"}) or \
                            soup.find("meta", {"property": "og:site_name"})
                            
                if journal_meta and journal_meta.get("content"):
                    journal_info["journal_name"] = journal_meta.get("content")
                    self.logger.info(f"Found journal name from HTML metadata: {journal_info['journal_name']}")
        # Ê†áÂáÜÂåñÊúüÂàäÂêçÁß∞
            if journal_info["journal_name"]:
                journal_info["journal_name"] = await self._normalize_journal_name(journal_info["journal_name"])
                self.logger.info(f"Normalized journal name: {journal_info['journal_name']}")
                              
            return journal_info
            
        except Exception as e:
            self.logger.warning(f"Error extracting journal info from URL: {e}")
            return journal_info

    async def _fetch_url_content(self, url, timeout=10):
        """Ëé∑ÂèñURLÂÜÖÂÆπÁöÑËæÖÂä©ÂáΩÊï∞"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = await asyncio.to_thread(
                lambda: requests.get(url, headers=headers, timeout=timeout)
            )
            
            if response.status_code == 200:
                return response.text
            return None
        except Exception as e:
            self.logger.warning(f"Error fetching URL {url}: {e}")
            return None


    #! Âä†++
    def _extract_arxiv_info_from_url(self, url):
        """‰ªéarXiv URL‰∏≠ÊèêÂèñ‰ø°ÊÅØ"""
        try:
            arxiv_info = {}
            # ‰ªéURL‰∏≠ÊèêÂèñarXiv ID
            arxiv_id_match = re.search(r'arxiv\.org/pdf/(\d{4})\.(.*?)$', url)
            if not arxiv_id_match:
                arxiv_id_match = re.search(r'\.([^./]+)$', url)  # ÂåπÈÖçÊúÄÂêé‰∏Ä‰∏™ÁÇπÂêéÈùûÊñúÊù†ÂÜÖÂÆπ
                arxiv_id = arxiv_id_match.group(1)
                if not arxiv_id_match:
                    arxiv_id = "Unknown document"  # Êó†Ê≥ïÊèêÂèñarxiv IDÔºåËøîÂõûÁ©∫Â≠óÁ¨¶‰∏≤
            else:
                arxiv_id = arxiv_id_match.group(2)

            arxiv_info["arxiv_id"] = arxiv_id
            arxiv_info["published_date"] = self.extract_and_convert_arxiv_date(url)

            return arxiv_info

        except Exception as e:
            self.logger.warning(f"Error extracting arXiv info from URL: {e}")

    def extract_and_convert_arxiv_date(self,url: str) -> str:
        """
        ‰ªéarXivÈìæÊé•‰∏≠ÊèêÂèñÊó•ÊúüÊ†áËÆ∞Âπ∂ËΩ¨Êç¢‰∏∫ÊúüÂæÖÊó•ÊúüÊ†ºÂºè

        ÂèÇÊï∞:
        url (str): arXivËÆ∫ÊñáÈìæÊé•ÔºåÂ¶Ç"http://arxiv.org/pdf/2410.15367v1"

        ËøîÂõû:
        str: ÊúüÂæÖÊó•ÊúüÊ†ºÂºèÔºöÂ¶Ç"2024.10"
        """
        # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÂåπÈÖçarxiv ID‰∏≠ÁöÑÊó•ÊúüÈÉ®ÂàÜÔºàÂ¶Ç2410Êàñ1704Ôºâ
        match = re.search(r'arxiv\.org/pdf/(\d{2})(\d{2})', url, re.IGNORECASE)
        if not match:
            match = re.search(r'\.(\d{2})(\d{2})', url[::-1])
            if match:
                digits_m = match.group(1)[::-1]
                digits_y = match.group(2)[::-1]
                digits = digits_y + digits_m  # ÂêàÂπ∂Âπ¥ÂíåÊúà
            else:
                return ""  # Êó†Ê≥ïÊèêÂèñÊó•ÊúüÔºåËøîÂõûÂéüURL
        else:
            # ÊèêÂèñÂπ¥‰ªΩÂíåÊúà‰ªΩ
            digits_y, digits_m = match.groups()

        # Â∞Ü2‰ΩçÂπ¥‰ªΩËΩ¨Êç¢‰∏∫4‰ΩçÔºàÂÅáËÆæ2000Âπ¥‰πãÂêéÔºâ
        year = int(digits_y)
        year_full = 2000 + year if year < 50 else 1900 + year

        # ÊûÑÂª∫ÊúüÂæÖÊó•Êúü
        try:
            # Á°Æ‰øùÊúà‰ªΩÂú®ÊúâÊïàËåÉÂõ¥Ôºà1-12Ôºâ
            month = int(digits_m)
            if 1 <= month <= 12:
                # return f"{year_full}.{month:02d}"  # Áõ¥Êé•Ê†ºÂºèÂåñÂ≠óÁ¨¶‰∏≤
                return f"{year_full}"
            else:
                return f"{year_full}"  # Êó†ÊïàÊúà‰ªΩÔºåÂè™ÂèëÂπ¥‰ªΩ
        except ValueError:
            return f"{year_full}-{digits_m.zfill(2)}"  # ÂºÇÂ∏∏Â§ÑÁêÜÔºå‰øùÊåÅÂéüÂßãÊï∞Â≠ó