import asyncio
import random
import json
import re
import html
from typing import Dict, Optional, List, Any, AsyncGenerator
import logging
import os
from gpt_researcher.llm_provider.generic.base import GenericLLMProvider
from ..actions.utils import stream_output
from ..actions.query_processing import plan_research_outline, get_search_results, generate_pubmed_sub_queries
from ..document import DocumentLoader, OnlineDocumentLoader, LangChainDocumentLoader
from ..utils.enum import ReportSource, ReportType, Tone
from ..utils.logging_config import get_json_handler, get_research_logger
from pathlib import Path
import markdown
from weasyprint import HTML
import openai
import numpy as np
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import requests
import pandas as pd
from ..retrievers import PubmedDianSearch


authors_t = 1

class ResearchConductor:
    """Manages and coordinates the research process."""

    def __init__(self, researcher):
        self.researcher = researcher
        self.logger = logging.getLogger('research')
        self.json_handler = get_json_handler()

    async def plan_research(self, query, query_domains=None):
        self.logger.info(f"Planning research for query: {query}")
        if query_domains:
            self.logger.info(f"Query domains: {query_domains}")
        
        await stream_output(
            "logs",
            "planning_research",
            f"ğŸŒ Browsing the web to learn more about the task: {query}...",
            self.researcher.websocket,
        )

        search_results = await get_search_results(query, self.researcher.retrievers[0], query_domains)
        self.logger.info(f"Initial search results obtained: {len(search_results)} results")

        await stream_output(
            "logs",
            "planning_research",
            f"ğŸ¤” Planning the research strategy and subtasks...",
            self.researcher.websocket,
        )


        outline = await plan_research_outline(
            query=query,
            search_results=search_results,
            agent_role_prompt=self.researcher.role,
            cfg=self.researcher.cfg,
            parent_query=self.researcher.parent_query,
            report_type=self.researcher.report_type,
            cost_callback=self.researcher.add_costs
        )
        self.logger.info(f"Research outline planned: {outline}")
        return outline

    async def conduct_research(self):
        """Runs the GPT Researcher to conduct research"""
        if self.json_handler:
            self.json_handler.update_content("query", self.researcher.query)
        
        self.logger.info(f"Starting research for query: {self.researcher.query}")
        self.logger.info(f"Report type: {self.researcher.report_type}")
        
        # å¦‚æœæ˜¯ä¸»æŸ¥è¯¢ï¼ˆéå­ä¸»é¢˜æŠ¥å‘Šï¼‰ï¼Œé‡ç½®ç»“æœ
        if self.researcher.report_type != "subtopic_report":
            self.logger.info(f"Main query detected, resetting accumulated results")

            # self.researcher.accumulated_classified_results = {
            #     "arxiv": [],
            #     "pubmed": [],
            #     "tavily": []
            # }

            #todo ä¿®+ï¼ˆè¯•ï¼‰
            self.researcher.accumulated_classified_results = {}



        else:
            self.logger.info(f"Subtopic report detected, using existing accumulated results")
            if not hasattr(self.researcher, 'accumulated_classified_results'):
                self.logger.error("No existing accumulated results found for subtopic report")
                raise AttributeError("accumulated_classified_results not found for subtopic report")
        
        # Reset visited_urls and source_urls at the start of each research task
        self.researcher.visited_urls.clear()
        research_data = []

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "starting_research",
                f"ğŸ” Starting the research task for '{self.researcher.query}'...",
                self.researcher.websocket,
            )

        if self.researcher.verbose:
            await stream_output("logs", "agent_generated", self.researcher.agent, self.researcher.websocket)

        # Research for relevant sources based on source types below
        if self.researcher.source_urls:
            self.logger.info("Using provided source URLs")
            research_data = await self._get_context_by_urls(self.researcher.source_urls)
            if research_data and len(research_data) == 0 and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "answering_from_memory",
                    f"ğŸ§ I was unable to find relevant context in the provided sources...",
                    self.researcher.websocket,
                )
            if self.researcher.complement_source_urls:
                self.logger.info("Complementing with web search")
                additional_research = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
                research_data += ' '.join(additional_research)

        elif self.researcher.report_source == ReportSource.Web.value:
            self.logger.info("Using web search")
            research_data = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains, self.researcher.accumulated_classified_results)

        # ... rest of the conditions ...
        elif self.researcher.report_source == ReportSource.Local.value:
            self.logger.info("Using local search")
            document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            self.logger.info(f"Loaded {len(document_data)} documents")
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)

            research_data = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)

        # Hybrid search including both local documents and web sources
        elif self.researcher.report_source == ReportSource.Hybrid.value:
            if self.researcher.document_urls:
                document_data = await OnlineDocumentLoader(self.researcher.document_urls).load()
            else:
                document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)
            docs_context = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)
            web_context = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
            research_data = f"Context from local documents: {docs_context}\n\nContext from web sources: {web_context}"

        elif self.researcher.report_source == ReportSource.Azure.value:
            from ..document.azure_document_loader import AzureDocumentLoader
            azure_loader = AzureDocumentLoader(
                container_name=os.getenv("AZURE_CONTAINER_NAME"),
                connection_string=os.getenv("AZURE_CONNECTION_STRING")
            )
            azure_files = await azure_loader.load()
            document_data = await DocumentLoader(azure_files).load()  # Reuse existing loader
            research_data = await self._get_context_by_web_search(self.researcher.query, document_data)
            
        elif self.researcher.report_source == ReportSource.LangChainDocuments.value:
            langchain_documents_data = await LangChainDocumentLoader(
                self.researcher.documents
            ).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(langchain_documents_data)
            research_data = await self._get_context_by_web_search(
                self.researcher.query, langchain_documents_data, self.researcher.query_domains
            )

        elif self.researcher.report_source == ReportSource.LangChainVectorStore.value:
            research_data = await self._get_context_by_vectorstore(self.researcher.query, self.researcher.vector_store_filter)

        # Rank and curate the sources
        self.researcher.context = research_data
        if self.researcher.cfg.curate_sources:
            self.logger.info("Curating sources")
            self.researcher.context = await self.researcher.source_curator.curate_sources(research_data)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "research_step_finalized",
                f"Finalized research step.\nğŸ’¸ Total Research Costs: ${self.researcher.get_costs()}",
                self.researcher.websocket,
            )
            if self.json_handler:
                self.json_handler.update_content("costs", self.researcher.get_costs())
                self.json_handler.update_content("context", self.researcher.context)

        self.logger.info(f"Research completed. Context size: {len(str(self.researcher.context))}")
        return self.researcher.context

    async def _get_context_by_urls(self, urls):
        """Scrapes and compresses the context from the given urls"""
        self.logger.info(f"Getting context from URLs: {urls}")
        
        new_search_urls = await self._get_new_urls(urls)
        self.logger.info(f"New URLs to process: {new_search_urls}")

        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)
        self.logger.info(f"Scraped content from {len(scraped_content)} URLs")

        if self.researcher.vector_store:
            self.logger.info("Loading content into vector store")
            self.researcher.vector_store.load(scraped_content)

        context = await self.researcher.context_manager.get_similar_content_by_query(
            self.researcher.query, scraped_content
        )
        return context

    # Add logging to other methods similarly...

    async def _get_context_by_vectorstore(self, query, filter: Optional[dict] = None):
        """
        Generates the context for the research task by searching the vectorstore
        Returns:
            context: List of context
        """
        self.logger.info(f"Starting vectorstore search for query: {query}")
        context = []
        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query)
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸  I will conduct my research based on the following queries: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # Using asyncio.gather to process the sub_queries asynchronously
        context = await asyncio.gather(
            *[
                self._process_sub_query_with_vectorstore(sub_query, filter)
                for sub_query in sub_queries
            ]
        )
        return context

    async def _calculate_query_similarity(self, main_query, sub_query):
        """è®¡ç®—ä¸»æŸ¥è¯¢å’Œå­æŸ¥è¯¢çš„ç›¸ä¼¼åº¦"""
        main_embedding = await self.researcher.memory.get_embeddings().aembed_query(main_query)
        sub_embedding = await self.researcher.memory.get_embeddings().aembed_query(sub_query)
        similarity = np.dot(main_embedding, sub_embedding) / (
            np.linalg.norm(main_embedding) * np.linalg.norm(sub_embedding)
        )
        return similarity

    def _calculate_context_rank_score(self, total_results, current_rank):
        """è®¡ç®—ä¸Šä¸‹æ–‡æ’åºå¾—åˆ†"""
        return (total_results - current_rank + 1) / total_results

    def _calculate_source_authority_score(self, source):
        """è®¡ç®—æ¥æºæƒå¨æ€§å¾—åˆ†"""
        source_scores = {
            "pubmed": 5,
            "arxiv": 3,
            "tavily": 2
        }
        return source_scores.get(source.lower(), 0)

    async def _get_context_by_web_search(self, query, scraped_data: list = [], query_domains: list = [], accumulated_classified_results: dict = None):
        """
        Generates the context for the research task by searching the query and scraping the results
        Returns:
            context: List of context
        """
        self.logger.info(f"Starting web search for query: {query}")
        
        # è½½å…¥æœŸåˆŠå½±å“å› å­Excelæ•°æ®
        journal_df = None
        excel_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "data/journal_impact_factors.xlsx")
        try:
            journal_df = pd.read_excel(excel_path)
            self.logger.info(f"Successfully loaded journal impact factors from {excel_path}")
            self.logger.info(f"Loaded {len(journal_df)} journal entries")
        except Exception as e:
            self.logger.warning(f"Could not load journal impact factors: {e}. Will use default scores.")
        
        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query, query_domains)
        self.logger.info(f"Generated sub-queries: {sub_queries}")
        
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            mark_query = "\n" + "".join([f"## {q.strip()}\n" for q in sub_queries])
            await stream_output(
                "logs",
                "mark_query",
                f"# {query}{mark_query}",
                self.researcher.websocket,
            )
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸ I will conduct my research based on the following queries: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # æµ‹è¯•ç”¨ï¼š
        self.logger.info(f"å…¨éƒ¨çš„æŸ¥è¯¢queryçš„å†…å®¹æ˜¯ï¼š{sub_queries}")

        # è·å–åŸå§‹æœç´¢ç»“æœ
        context = await asyncio.gather(
            *[
                self._process_sub_query(sub_query, scraped_data, query_domains)
                for sub_query in sub_queries
            ]
        )


        # åŠ +
        min_jif = self.researcher.min_jif_score  # éœ€æ·»åŠ æœ€ä½æœŸå¾…å½±å“å› å­å¾—åˆ†çš„å­—æ®µ

        #todo ä¿®+
        source_type_keys = ["tavily", "arxiv", "pubmed"]  # å®šä¹‰æ¥æºç±»å‹é”®

        # è®°å½•åŸå§‹æœç´¢ç»“æœ
        self.logger.info("raw search results:")
        for idx, contents in enumerate(context):
            if contents:
                self.logger.info(f"subquery {idx + 1} - '{sub_queries[idx]}' research results:")
                # å°†contentsæŒ‰å—åˆ†å‰²
                content_blocks = contents.split("\n\n")
                self.logger.info(f"find {len(content_blocks)} content_blocks")
                self.logger.info("-" * 50)
        
        # å¤„ç†æ‰€æœ‰æœç´¢ç»“æœå¹¶è®¡ç®—å¾—åˆ†
        try:
            all_scored_items = []

            # ç›´æ¥éå†contextåˆ—è¡¨
            for idx, contents in enumerate(context):
                if not contents:
                    continue
                
                # å°†å†…å®¹æŒ‰å—åˆ†å‰²
                content_blocks = contents.split("\n\n")

                # éå†è¯¥æŸ¥è¯¢çš„æ‰€æœ‰å†…å®¹å—
                for block in content_blocks:
                    if not block.strip():
                        continue

                    # è§£æå—å†…å®¹
                    source_match = re.search(r'^Source: (https?://[^\s]+)', block, re.M)
                    title_match = re.search(r'Title: (.+)', block)
                    content_match = re.search(r'Content: (.+)', block, re.DOTALL)
                    
                    if not all([source_match, title_match, content_match]):
                        continue
                        
                    url = source_match.group(1)
                    title = title_match.group(1).strip()
                    content_text = content_match.group(1).strip()



                    # ç¡®å®šæ¥æºé»˜è®¤ç±»å‹
                    source_type = source_type_keys[0]

                    # ä»å†…å®¹å—ä¸­è·å–æ£€ç´¢å™¨ç±»å‹
                    retriever_type_match = re.search(r'RetrieverType: (\w+)', block)
                    if retriever_type_match:
                        source_type = retriever_type_match.group(1)
                        self.logger.info(f"get source_type from content_block: {source_type}")  # æ‰“å°æ£€ç´¢å™¨ç±»å‹
                    else:
                        # ä»scraped_dataä¸­è·å–æ£€ç´¢å™¨ç±»å‹
                        for item in scraped_data:
                            if item.get('url') == url and 'retriever_type' in item:  # æ£€æŸ¥URLå’Œæ£€ç´¢å™¨ç±»å‹æ˜¯å¦å­˜åœ¨
                                source_type = item['retriever_type']  # è·å–æ£€ç´¢å™¨ç±»å‹
                                self.logger.info(f"get source_type from scraped_data: {source_type}")
                                break

                    # ä¿®+
                    # æ£€æŸ¥æ¥æºç±»å‹æ˜¯å¦æœ‰æ•ˆ
                    if source_type not in source_type_keys:  # å¦‚æœæ¥æºç±»å‹æ— æ•ˆï¼Œåˆ™è·³è¿‡æˆ–è€…å¼ºåˆ¶ä¿®æ”¹ä¸ºé»˜è®¤ç±»å‹
                        # continue
                        # å¼ºåˆ¶ä¿®æ”¹ä¸ºé»˜è®¤ç±»å‹
                        source_type = source_type_keys[0]  # é»˜è®¤ç±»å‹
                        self.logger.info(
                            f"Invalid source_type: {source_type}. Forcing to default: {source_type_keys[0]}")


                    # å¦‚æœæ¥æºç±»å‹æ˜¯tavilyï¼Œæ ¹æ®URLç‰¹å¾è¿›è¡ŒäºŒæ¬¡åˆ†ç±»
                    if source_type == source_type_keys[0]: # ä¿®+
                        if 'arxiv' in url.lower():  # å¦‚æœURLåŒ…å«arxivï¼Œåˆ™å°†æ¥æºç±»å‹æ”¹ä¸ºarxiv
                            source_type = "arxiv"
                            self.logger.info("according to url, change source_type from tavily to arxiv")
                        elif 'ncbi' in url.lower() or 'pubmed' in url.lower():  # å¦‚æœURLåŒ…å«ncbiæˆ–pubmedï¼Œåˆ™å°†æ¥æºç±»å‹æ”¹ä¸ºpubmed
                            source_type = "pubmed"
                            self.logger.info("according to url, change source_type from tavily to pubmed")

                    # è®¡ç®—æ¥æºæƒå¨æ€§å¾—åˆ†
                    source_authority_score = self._calculate_source_authority_score(source_type)
                    
                    # è®¡ç®—ä¸Šä¸‹æ–‡æ’åºå¾—åˆ† - ä½¿ç”¨å½“å‰ç»“æœåœ¨æ‰€æœ‰ç»“æœä¸­çš„ä½ç½®
                    context_rank_score = self._calculate_context_rank_score(len(all_scored_items) + 1, len(all_scored_items))

                    # è®¡ç®—å†…å®¹ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
                    content_similarity = await self._calculate_query_similarity(query, sub_queries[idx])  

                    impact_factor = 0


                    self.logger.info(f'our min_jif_score is {min_jif}')


                    journal_name = ""
                    if source_type == "pubmed" or source_type == "tavily":
                        # æå–æœŸåˆŠä¿¡æ¯
                        journal_info = await self._extract_journal_info_from_url(url)
                        journal_name = journal_info.get("journal_name", "")

                        self.logger.info(f"our journal_name is <<{journal_name}>> in this time")
                        # æŸ¥æ‰¾æœŸåˆŠå½±å“å› å­
                        if journal_df is not None and journal_name:
                            try:
                                self.logger.info("We are starting to search for journal factors ")
                                # æ ‡å‡†åŒ–æŸ¥è¯¢çš„æœŸåˆŠåç§°
                                normalized_journal_name = await self._normalize_journal_name(journal_name)

                                # å°è¯•ç²¾ç¡®åŒ¹é…
                                # é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ ‡å‡†åŒ–çš„æœŸåˆŠåç§°åˆ—
                                journal_df['NormalizedName'] = journal_df['Name'].apply(
                                    lambda x: str(x).replace(' - ', '-').replace(' And ', ' & ').replace(' and ', ' & ').replace('&amp;', '&').replace('&AMP;', '&') if not pd.isna(x) else ""
                                )
                                journal_match = journal_df[journal_df['NormalizedName'].str.upper() == normalized_journal_name.upper()]



                                # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
                                if journal_match.empty:

                                    self.logger.info("Liang__is tryingã€‹ã€‹ã€‹ã€‹ journal_math is also empty")

                                    for _, row in journal_df.iterrows():
                                        normalized_db_name = row['NormalizedName'].upper()
                                        
                                        if normalized_db_name in normalized_journal_name.upper() or normalized_journal_name.upper() in normalized_db_name:
                                            journal_match = pd.DataFrame([row])
                                            break
                                            
                                        # ä¹Ÿæ£€æŸ¥ç¼©å†™å
                                        if 'AbbrName' in row and not pd.isna(row['AbbrName']):
                                            abbr_name = str(row['AbbrName']).upper()
                                            normalized_abbr_name = await self._normalize_journal_name(abbr_name)
                                            
                                            if normalized_abbr_name in normalized_journal_name.upper() or normalized_journal_name.upper() in normalized_abbr_name:
                                                journal_match = pd.DataFrame([row])
                                                break
                                
                                # å¦‚æœæœ‰ISSNåŒ¹é…
                                if journal_match.empty and 'issn' in journal_info:

                                    self.logger.info("æœŸåˆŠåŒ¹é…ä¾ç„¶æ²¡æœ‰æˆåŠŸï¼ï¼")

                                    issn = journal_info['issn']
                                    journal_match = journal_df[(journal_df['ISSN'] == issn) | (journal_df['EISSN'] == issn)]
                                
                                if not journal_match.empty:

                                    impact_factor = float(journal_match.iloc[0]['JIF']) if 'JIF' in journal_match.columns and not pd.isna(journal_match.iloc[0]['JIF']) else 0
                                    self.logger.info(f"æœŸåˆŠ '{journal_name}' çš„å½±å“å› å­ (JIF): {impact_factor}")
                            except Exception as e:
                                self.logger.warning(f"æœŸåˆŠå½±å“å› å­æŸ¥è¯¢é”™è¯¯: {e}")


                    if impact_factor < min_jif:  # å¦‚æœå½±å“å› å­å°äºæœ€ä½æœŸå¾…å½±å“å› å­ï¼Œåˆ™è·³è¿‡
                       continue


                    # è·å–å¯¹åº”æ–‡çŒ®çš„å‘è¡¨å¹´ã€å·æœŸã€é¡µç ï¼ˆè‹¥æ²¡æœ‰ï¼Œåªç»™å¹´ä»½å°±å¥½ï¼‰
                    published_date = ""  # åˆå§‹åŒ–å‘è¡¨æ—¥æœŸ
                    authors = ""  # åˆå§‹åŒ–ä½œè€…
                    vol = ""  # åˆå§‹åŒ–å·å·
                    pagination = ""  # åˆå§‹åŒ–é¡µç 
                    if source_type == source_type_keys[2]:  # å¦‚æœæ¥æºç±»å‹æ˜¯pubmed

                        published_date_match = re.search(r'Published Date: (.+)', block)  # åŒ¹é…å‘è¡¨æ—¥æœŸ
                        vol_match = re.search(r'Volume: (.+)', block)  # åŒ¹é…å·å·
                        pagination_match = re.search(r'Pagination: (.+)', block)  # åŒ¹é…é¡µç 
                        authors_match = re.search(r'Authors: (.+)', block)  # åŒ¹é…ä½œè€…
                        # æå–ä¿¡æ¯
                        if authors_match:
                            self.logger.info("ä½œè€…åŒ¹é…äº†ï¼ï¼")
                            authors = authors_match.group(1).strip()  # è·å–ä½œè€…
                        if pagination_match:
                            self.logger.info("é¡µç åŒ¹é…äº†ï¼ï¼")
                            pagination = pagination_match.group(1).strip()  # è·å–é¡µç 
                        if vol_match:
                            self.logger.info("å·å·åŒ¹é…äº†ï¼ï¼")
                            vol = vol_match.group(1).strip()  # è·å–å·å·
                        if published_date_match:
                            self.logger.info("æ—¥æœŸåŒ¹é…äº†ï¼ï¼")
                            published = published_date_match.group(1).strip()  # è·å–å‘è¡¨æ—¥æœŸ
                            # published_date = published.split(" ")[0] # æå–å‘è¡¨æ—¥æœŸ
                            published_date = published[:4]

                    elif source_type == source_type_keys[1]:  # å¦‚æœæ¥æºç±»å‹æ˜¯arxiv
                        arxiv_info = self._extract_arxiv_info_from_url(url)  # æå–arxivä¿¡æ¯
                        published_date = arxiv_info.get("published_date", "")  # è·å–å‘è¡¨æ—¥æœŸ
                        # # æ£€æŸ¥å‘è¡¨æ—¥æœŸæ˜¯å¦åœ¨2020å¹´ä¹‹å
                        # if published_date and int(published_date.split(".")[0]) < 2020:  # æ£€æŸ¥å‘è¡¨æ—¥æœŸæ˜¯å¦åœ¨2020å¹´ä¹‹å
                        #     continue  # å¦‚æœä¸æ˜¯ï¼Œåˆ™è·³è¿‡
                        authors = arxiv_info.get("arxiv_id", "Unknown ID")  # è·å–ä½œè€…ï¼ˆå…ˆç”¨IDä»£æ›¿ï¼‰


                    if vol:
                        if pagination:  # å¦‚æœæœ‰å·å·å’Œé¡µç 
                            vol_pagination = f"Vol.{vol}:{pagination}"  # æ‹¼æ¥å·å·å’Œé¡µç 
                        else:  # å¦‚æœåªæœ‰å·å·
                            vol_pagination = f"Vol.{vol}"  # æ‹¼æ¥å·å·
                    else:
                        vol_pagination = ""

                        # æ ‡å‡†åŒ–å½±å“å› å­å¾—åˆ†åˆ°0-1èŒƒå›´
                    # å‡è®¾æœ€é«˜å½±å“å› å­ä¸º100ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
                    max_impact_factor = 503.1
                    normalized_impact_factor = min(impact_factor / max_impact_factor, 1.0)
                    
                    # è®¡ç®—æ€»å¾—åˆ† (è°ƒæ•´æƒé‡ä»¥åæ˜ æ–°çš„è¯„åˆ†æ–¹å¼)
                    total_score = (
                        0.3 * content_similarity +      # å†…å®¹ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
                        0.2 * context_rank_score +     # ä¸Šä¸‹æ–‡æ’åºå¾—åˆ†
                        0.2 * source_authority_score + # æ¥æºæƒå¨æ€§å¾—åˆ†
                        0.3 * normalized_impact_factor # æœŸåˆŠå½±å“å› å­å¾—åˆ†
                    )
                    
                    self.logger.info(f"***the content is from: {source_type}***")
                    all_scored_items.append({
                        'content': content_text,
                        'source': url,
                        'title': title,
                        'journal_name': journal_name,
                        'source_type': source_type,
                        'similarity_score': content_similarity,  # æ›´æ–°ä¸ºå†…å®¹ç›¸ä¼¼åº¦
                        'context_rank_score': context_rank_score,
                        'source_authority_score': source_authority_score,
                        'impact_factor': impact_factor,
                        'normalized_impact_factor': normalized_impact_factor,
                        'score': total_score,
                        'published_date': published_date,   # å‘è¡¨æ—¥æœŸ
                        'authors': authors,   # ä½œè€…
                        'vol_pagination': vol_pagination,   # å·å·+é¡µç 

                    })
                
            # è®°å½•æ’åºå‰çš„å¾—åˆ†æƒ…å†µ
            # self.logger.info("\næ’åºå‰çš„å¾—åˆ†æƒ…å†µ:")
            # for idx, item in enumerate(all_scored_items):
            #     try:
            #         self.logger.info(f"  item {idx + 1}:")
            #         self.logger.info(f"  url_source: {item['source']}")
            #         self.logger.info(f"  journal_name: {item['journal_name']}")
            #         self.logger.info(f"  source_type: {item['source_type']}")
            #         # å¤„ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            #         title = item['title'].encode('utf-8', errors='ignore').decode('utf-8')
            #         self.logger.info(f"  æ ‡é¢˜: {title}")
            #         self.logger.info(f"  å†…å®¹é•¿åº¦: {len(item['content'])}")
            #         self.logger.info(f"  ç›¸ä¼¼åº¦å¾—åˆ†: {item['similarity_score']:.4f}")
            #         self.logger.info(f"  ä¸Šä¸‹æ–‡æ’åå¾—åˆ†: {item['context_rank_score']:.4f}")
            #         self.logger.info(f"  æ¥æºæƒå¨æ€§å¾—åˆ†: {item['source_authority_score']:.4f}")
            #         self.logger.info(f"  å½±å“å› å­: {item['impact_factor']}")
            #         self.logger.info(f"  æ ‡å‡†åŒ–å½±å“å› å­å¾—åˆ†: {item['normalized_impact_factor']:.4f}")
            #         self.logger.info(f"  æ€»åˆ†: {item['score']:.4f}")
            #         self.logger.info("-" * 50)
            #     except Exception as e:
            #         self.logger.warning(f"è®°å½•é¡¹ç›® {idx + 1} æ—¶å‡ºé”™: {str(e)}")
            #         continue

            # æµ‹è¯•ç”¨ï¼š
            self.logger.info(f'æœ¬æ¬¡çš„å½±å“å› å­æœ€ä½è¦æ±‚æ˜¯ï¼š{min_jif}')
            self.logger.info(f'æ€»å…±{len(all_scored_items)}ä¸ªè¢«æ”¶å½•çš„æ–‡çŒ®')



            # æŒ‰å¾—åˆ†æ’åº
            all_scored_items.sort(key=lambda x: x['score'], reverse=True)

            # self.logger.info(f'å±•ç¤ºè¢«æ”¶å½•é‡‡çº³çš„æ–‡çŒ®ä¿¡æ¯ï¼šï¼š')
            # for item in all_scored_items:
            #     try:
            #         self.logger.info(f"  item: {item['title']}")
            #         self.logger.info(f"  source: {item['source']}")
            #         self.logger.info(f"  journal_name: {item['journal_name']}")
            #         self.logger.info(f"  impact_factor: {item['impact_factor']}")
            #         self.logger.info(f"  source_type: {item['source_type']}")
            #         self.logger.info(f"  authors: {item['authors']}")
            #         self.logger.info(f"  published_date: {item['published_date']}")
            #         self.logger.info(f"  vol_pagination: {item['vol_pagination']}")
            #         self.logger.info(f'------------------------')
            #     except Exception as e:
            #         self.logger.warning(f"è®°å½•é¡¹ç›®æ—¶å‡ºé”™: {str(e)}")
            #         continue

            # # è®°å½•æ’åºåçš„ç»“æœ
            # self.logger.info("\næ’åºåçš„ç»“æœ:")
            # for idx, item in enumerate(all_scored_items):
            #     self.logger.info(f"æ’å {idx + 1}:")
            #     self.logger.info(f"  æ¥æº: {item['source']}")
            #     self.logger.info(f"  æœŸåˆŠåç§°: {item['journal_name']}")
            #     self.logger.info(f"  æ¥æºç±»å‹: {item['source_type']}")
            #     self.logger.info(f"  æ ‡é¢˜: {item['title']}")
            #     self.logger.info(f"  å†…å®¹é•¿åº¦: {len(item['content'])}")
            #     self.logger.info(f"  ç›¸ä¼¼åº¦å¾—åˆ†: {item['similarity_score']:.4f}")
            #     self.logger.info(f"  ä¸Šä¸‹æ–‡æ’åå¾—åˆ†: {item['context_rank_score']:.4f}")
            #     self.logger.info(f"  æ¥æºæƒå¨æ€§å¾—åˆ†: {item['source_authority_score']:.4f}")
            #     self.logger.info(f"  å½±å“å› å­: {item['impact_factor']}")
            #     self.logger.info(f"  æ ‡å‡†åŒ–å½±å“å› å­å¾—åˆ†: {item['normalized_impact_factor']:.4f}")
            #     self.logger.info(f"  æ€»åˆ†: {item['score']:.4f}")
            #     self.logger.info("-" * 50)
            
            # è®¾ç½®é˜ˆå€¼æˆ–å–å‰Nä¸ªç»“æœ
            threshold = 0.4  # é»˜è®¤é˜ˆå€¼
            max_results = 20  # æœ€å¤§ç»“æœæ•°é‡
            
            # åº”ç”¨é˜ˆå€¼æˆ–å–å‰Nä¸ª
            filtered_items = [item for item in all_scored_items if item['score'] >= threshold]
            if not filtered_items:  # å¦‚æœé˜ˆå€¼è¿‡æ»¤åæ²¡æœ‰ç»“æœï¼Œå–æ’åå‰Nçš„ç»“æœ
                filtered_items = all_scored_items[:max_results]
            elif len(filtered_items) > max_results:  # å¦‚æœç»“æœè¿‡å¤šï¼Œé™åˆ¶æ•°é‡
                filtered_items = filtered_items[:max_results]
            
            # è®°å½•æœ€ç»ˆé€‰æ‹©çš„ç»“æœ
            self.logger.info(f"\næœ€ç»ˆé€‰æ‹©çš„ç»“æœ (é˜ˆå€¼: {threshold}, æœ€å¤§æ•°é‡: {max_results}):")
            self.logger.info(f"é€‰æ‹©çš„é¡¹ç›®æ•°: {len(filtered_items)}")
            
            # æ ¼å¼åŒ–è¾“å‡ºï¼Œä¿æŒä¸åŸå§‹æ ¼å¼å…¼å®¹
            formatted_context = []
            for item in filtered_items:
                formatted_block = (
                    f"Source: {item['source']}\n"
                    f"Title: {item['title']}\n"
                    f"Content: {item['content']}\n"
                )
                formatted_context.append(formatted_block)

            # å°†filtered_itemsè½¬æ¢ä¸ºåˆ†ç±»æ ¼å¼

            #å›ºå®šåˆå§‹å€¼æ ¼å¼ï¼šåˆ—è¡¨
            initial_value = []  #

            classified_items = {key: initial_value.copy() for key in source_type_keys}

            for item in filtered_items:
                source = item['source']
                parsed_block = {
                    "source": source,
                    "JournalName": item['journal_name'],
                    "title": item['title'],
                    "content": item['content'],
                    # åŠ ++
                    'impact_factor': item['impact_factor'],
                    'published_date': item.get('published_date', ''),  # è·å–å‘å¸ƒæ—¥æœŸï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é»˜è®¤ä¸º'',
                    'authors': item.get('authors', ''),
                    'vol_pagination': item.get("vol_pagination")

                }
                
                # if item['source_type'] == "pubmed":
                #     classified_items['pubmed'].append(parsed_block)
                # elif item['source_type'] == "arxiv":
                #     classified_items['arxiv'].append(parsed_block)
                # else:
                #     classified_items['tavily'].append(parsed_block)

                #todo ä¿®+
                #æ£€æŸ¥æ¥æºç±»å‹å¹¶æ·»åŠ åˆ°å¯¹åº”çš„åˆ†ç±»ä¸­
                classified_items[item['source_type']].append(parsed_block)
            
            # accumulated_classified_results = accumulated_classified_results
            # # æ›´æ–°ç´¯ç§¯çš„åˆ†ç±»ç»“æœï¼Œä¿æŒåˆ†ç±»ç»“æ„
            # self.logger.info("Updating accumulated results")
            # for category in classified_items:
            #     try:
            #         # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„source
            #         existing_sources = {item['source'] for item in self.researcher.accumulated_classified_results[category]}
            #         # åªæ·»åŠ æ–°çš„source
            #         new_items = [item for item in classified_items[category] if item['source'] not in existing_sources]
            #         if new_items:
            #             self.logger.info(f"Adding {len(new_items)} new items to {category}")
            #             self.researcher.accumulated_classified_results[category].extend(new_items)
            #     except Exception as e:
            #         self.logger.error(f"Error processing category {category}: {str(e)}")
            #         continue

            #todo ä¿®+(è¯•)
            #æ›´æ–°ç´¯ç§¯çš„åˆ†ç±»ç»“æœï¼Œä¿æŒåˆ†ç±»ç»“æ„
            self.logger.info("Updating accumulated results")

            for category, context in classified_items.items():
                if not self.researcher.accumulated_classified_results.setdefault(category, None):
                    self.researcher.accumulated_classified_results[category] = []
                le_s = len(self.researcher.accumulated_classified_results[category])
                try:
                    for item_dict in context:
                        if len(self.researcher.accumulated_classified_results[category]) == 0:
                            self.researcher.accumulated_classified_results[category].append(item_dict)
                        else:
                            inf = True
                            for i in range(len(self.researcher.accumulated_classified_results[category])):
                                if self.researcher.accumulated_classified_results[category][i]['source'] == item_dict[
                                    'source']:
                                    self.researcher.accumulated_classified_results[category][i]['content'] += item_dict[
                                        'content']
                                    inf = False
                                    break
                            if inf:
                                self.researcher.accumulated_classified_results[category].append(item_dict)

                    le_e = len(self.researcher.accumulated_classified_results[category])

                    self.logger.info(f"Added {le_e - le_s} new items to {category}")

                except Exception as e:
                    self.logger.error(f"Error processing category {category}: {str(e)}")
                    continue


            # è®°å½•å½“å‰ç´¯ç§¯çš„ç»“æœ
            self.logger.info("Current accumulated results:")
            for category in self.researcher.accumulated_classified_results:
                self.logger.info(f"{category}: {len(self.researcher.accumulated_classified_results[category])} items")

            # # æµ‹è¯•ç”¨ï¼š
            # for category,item_list in self.researcher.accumulated_classified_results.items():
            #     self.logger.info(f"å±•ç¤ºæœç´¢å¼•æ“{category}: {len(item_list)} itemsï¼Œè·å–çš„å†…å®¹ï¼š"+"\n\n")
            #     for item in item_list:
            #         self.logger.info(f"{item}"+"\n")


            # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            self.logger.info("Converting to JSON")
            classified_json = json.dumps(self.researcher.accumulated_classified_results, ensure_ascii=False, indent=2)
            self.logger.info(f"åˆ†ç±»ç»“æœ: {classified_json}") #
            await stream_output(
                    "logs", "subquery_context_window", f"{classified_json}", self.researcher.websocket
                )
            if formatted_context:
                combined_context = " ".join(formatted_context)
                self.logger.info(f"æœ€ç»ˆç»„åˆä¸Šä¸‹æ–‡å¤§å°: {len(combined_context)}")
                return combined_context
            return []
            
        except Exception as e:
            self.logger.error(f"Error during web search: {e}", exc_info=True)
            import traceback
            self.logger.error(traceback.format_exc())
            return []

    async def _process_sub_query(self, sub_query: str, scraped_data: list = [], query_domains: list = []):
        """Takes in a sub query and scrapes urls based on it and gathers context."""
        if self.json_handler:
            self.json_handler.log_event("sub_query", {
                "query": sub_query,
                "scraped_data_size": len(scraped_data)
            })
        
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_research",
                f"\nğŸ” Running research for '{sub_query}'...",
                self.researcher.websocket,
            )

        try:
            if not scraped_data:
                scraped_data = await self._scrape_data_by_urls(sub_query, query_domains)
                self.logger.info(f"Scraped data size: {len(scraped_data)}")

            content = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)
            self.logger.info(f"Content found for sub-query: {len(str(content)) if content else 0} chars")
            
            # è§£æå†…å®¹å—å¹¶æ·»åŠ æ£€ç´¢å™¨ç±»å‹ä¿¡æ¯
            if content:
                content_blocks = re.split(r'\n(?=Source: https?://)', content.strip())
                processed_blocks = []
                
                for block in content_blocks:
                    if not block.strip():
                        continue
                        
                    # è§£æå—å†…å®¹
                    source_match = re.search(r'^Source: (https?://[^\s]+)', block, re.M)
                    title_match = re.search(r'Title: (.+)', block)
                    content_match = re.search(r'Content: (.+)', block, re.DOTALL)
                    
                    if not all([source_match, title_match, content_match]):
                        continue
                        
                    url = source_match.group(1)
                    title = title_match.group(1).strip()
                    content_text = content_match.group(1).strip()
                    
                    # ä»scraped_dataä¸­è·å–æ£€ç´¢å™¨ç±»å‹
                    retriever_type = "tavily"  # é»˜è®¤ç±»å‹
                    for item in scraped_data:
                        if item.get('url') == url and 'retriever_type' in item:
                            retriever_type = item['retriever_type']
                            self.logger.info(f"ä»scraped_dataä¸­è·å–åˆ°æ£€ç´¢å™¨ç±»å‹: {retriever_type}")
                            break


                    # åŠ ++
                    # ä»scraped_dataä¸­è·å–å‡ºç‰ˆæ—¶é—´ä¿¡æ¯
                    published_date = ""  # é»˜è®¤å‡ºç‰ˆæ—¶é—´ä¸ºç©º
                    for item in scraped_data:
                        if item.get('url') == url and 'published_date' in item:
                            published_date = item['published_date']
                            self.logger.info(f"ä»scraped_dataä¸­è·å–åˆ°å‡ºç‰ˆæ—¶é—´: {published_date}")
                            break

                    # ä»scraped_dataä¸­è·å–ä½œè€…ä¿¡æ¯
                    authors = ""  # é»˜è®¤ä½œè€…ä¸ºç©º
                    for item in scraped_data:
                        if item.get('url') == url and 'authors' in item:
                            authors = item['authors']
                            self.logger.info(f"ä»scraped_dataä¸­è·å–åˆ°ä½œè€…ä¿¡æ¯: {authors}")
                            break

                    if authors and isinstance(authors, list):  # å¦‚æœä½œè€…ä¿¡æ¯å­˜åœ¨,
                        author_names = ""
                        # æå–ä½œè€…çš„åå­—
                        for idx in range(len(authors)):
                            if idx > authors_t - 1:
                                break
                            author_names += authors[idx] + ", "
                        authors = author_names[:-2] + ", et al."
                        self.logger.info(f"æœ€åæå–åˆ°çš„ä½œè€…ä¿¡æ¯: {authors}")

                    # ä»scraped_dataä¸­è·å–å·å·ä¿¡æ¯
                    vol = ""  # é»˜è®¤å·å·ä¸ºç©º
                    for item in scraped_data:
                        if item.get('url') == url and 'vol' in item:
                            vol = item['vol']
                            self.logger.info(f"ä»scraped_dataä¸­è·å–åˆ°å·å·ä¿¡æ¯: {vol}")
                            break

                    # ä»scraped_dataä¸­è·å–é¡µç ä¿¡æ¯
                    pagination = ""  # é»˜è®¤é¡µç ä¸ºç©º
                    for item in scraped_data:
                        if item.get('url') == url and 'pagination' in item:
                            pagination = item['pagination']
                            self.logger.info(f"ä»scraped_dataä¸­è·å–åˆ°é¡µç ä¿¡æ¯: {pagination}")
                            break

                    # ä¿®++
                    # æ„å»ºæ–°çš„å†…å®¹å—ï¼ŒåŒ…å«æ£€ç´¢å™¨ç±»å‹ä¿¡æ¯
                    processed_block = (
                        f"Source: {url}\n"
                        f"Title: {title}\n"
                        f"Content: {content_text}\n"
                        f"RetrieverType: {retriever_type}\n"
                        f"Published Date: {published_date}\n"
                        f"Volume: {vol}\n"
                        f"Pagination: {pagination}\n"
                        f"Authors: {authors}\n"

                    )
                    processed_blocks.append(processed_block)
                
                # é‡æ–°ç»„åˆå¤„ç†åçš„å†…å®¹
                content = "\n".join(processed_blocks)

            if content and self.researcher.verbose:
                print(f"Content found 12345")
            elif self.researcher.verbose:
                await stream_output(
                    "logs",
                    "subquery_context_not_found",
                    f"ğŸ¤· No content found for '{sub_query}'...",
                    self.researcher.websocket,
                )
            if content:
                if self.json_handler:
                    self.json_handler.log_event("content_found", {
                        "sub_query": sub_query,
                        "content_size": len(content)
                    })

            return content
        except Exception as e:
            self.logger.error(f"Error processing sub-query {sub_query}: {e}", exc_info=True)
            return ""


    async def _process_sub_query_with_vectorstore(self, sub_query: str, filter: Optional[dict] = None):
        """Takes in a sub query and gathers context from the user provided vector store

        Args:
            sub_query (str): The sub-query generated from the original query

        Returns:
            str: The context gathered from search
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_with_vectorstore_research",
                f"\nğŸ” Running research for '{sub_query}'...",
                self.researcher.websocket,
            )

        content = await self.researcher.context_manager.get_similar_content_by_query_with_vectorstore(sub_query, filter)

        if content and self.researcher.verbose:
            await stream_output(
                "logs", "subquery_context_window", f"ğŸ“ƒ {content}", self.researcher.websocket
            )
        elif self.researcher.verbose:
            await stream_output(
                "logs",
                "subquery_context_not_found",
                f"ğŸ¤· No content found for '{sub_query}'...",
                self.researcher.websocket,
            )
        return content

    async def classify_content(self, content: str) -> Dict[str, List[Dict]]:
        """
        å¼‚æ­¥åˆ†ç±»å†…å®¹åˆ°ä¸åŒç±»åˆ«ï¼ˆarxiv/tavilyï¼‰
    
         Args:
            content: åŒ…å«å¤šä¸ªå†…å®¹å—çš„åŸå§‹æ–‡æœ¬
        
        Returns:
        åˆ†ç±»åçš„å­—å…¸ç»“æ„ {
            "arxiv": [åŒ…å«arxivçš„å—],
            "tavily": [å…¶ä»–å—]
        }
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å†…å®¹å—
        blocks = re.split(r'\n(?=Source: https?://)', content.strip())
        
        classified = {"arxiv": [], "pubmed": [], "tavily": []}  # åˆå§‹åŒ–æ‰€æœ‰å¯èƒ½çš„ç±»åˆ«
        
        for block in blocks:
            if not block.strip():
                continue
                
            # è§£æå—å†…å®¹
            source_match = re.search(r'^Source: (https?://[^\s]+)', block, re.M)
            title_match = re.search(r'Title: (.+)', block)
            content_match = re.search(r'Content: (.+)', block, re.DOTALL)
            
            if not all([source_match, title_match, content_match]):
                continue
                
            parsed_block = {
                "source": source_match.group(1), 
                "title": title_match.group(1).strip().replace('\n', ''),
                "content": content_match.group(1).strip().replace('\n', ' ')
            }

            # åˆ†ç±»é€»è¾‘
            if 'arxiv' in parsed_block['source'].lower():
                classified['arxiv'].append(parsed_block)
            elif 'ncbi' in parsed_block['source'].lower():
                classified['pubmed'].append(parsed_block)
            else:
                classified['tavily'].append(parsed_block)

        # å°†æ²¡æœ‰å†…å®¹çš„åˆ†ç±»è¿›è¡Œå‰”é™¤ï¼Œä¸åœ¨è¾“å‡ºå±•ç¤º
        classified = {key: value for key, value in classified.items() if value}
        # ä½¿ç”¨ json.dumps å°†å­—å…¸è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼Œç¡®ä¿ä½¿ç”¨åŒå¼•å·
        classified_json = json.dumps(classified, ensure_ascii=False, indent=2)

        return classified_json

    async def _get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """

        new_urls = []
        for url in url_set_input:
            if url not in self.researcher.visited_urls:
                self.researcher.visited_urls.add(url)
                new_urls.append(url)
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "added_source_url",
                        f"âœ… Added source url to research: {url}\n",
                        self.researcher.websocket,
                        True,
                        url,
                    )

        return new_urls

    async def _search_relevant_source_urls(self, query, query_domains: list = []):
        new_search_urls = []
        search_results_with_type = []  # å­˜å‚¨å¸¦æ£€ç´¢å™¨ç±»å‹çš„ç»“æœ

        # éå†æ‰€æœ‰æ£€ç´¢å™¨
        for retriever_class in self.researcher.retrievers:
            # è·å–å½“å‰æ£€ç´¢å™¨ç±»å‹
            current_retriever_type = retriever_class.__name__.lower() 
            # æ˜ å°„æ£€ç´¢å™¨ç±»å‹
            retriever_type_mapping = {
                'tavilysearch': 'tavily',
                'arxivsearch': 'arxiv',
                'pubmeddiansearch': 'pubmed'  # ä½¿ç”¨å°å†™å½¢å¼
            }
            current_retriever_type = retriever_type_mapping.get(current_retriever_type, 'tavily')
            self.logger.info(f"Current retriever type: {current_retriever_type} (original: {retriever_class.__name__})")
            
            # æ ¹æ®æ£€ç´¢å™¨ç±»å‹å¤„ç†æŸ¥è¯¢
            processed_query = query
            if current_retriever_type == "pubmed":
                # å¯¹ PubMed æ£€ç´¢å™¨è¿›è¡Œç‰¹æ®Šå¤„ç†
                self.logger.info("Processing query for PubMed retriever")
                # ä½¿ç”¨ generate_pubmed_sub_queries ç”Ÿæˆè§„èŒƒåŒ–çš„æŸ¥è¯¢
                sub_queries = await generate_pubmed_sub_queries(
                    query=query,
                    cfg=self.researcher.cfg,
                    cost_callback=self.researcher.add_costs
                )
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå­æŸ¥è¯¢ä½œä¸ºä¸»è¦æŸ¥è¯¢
                if sub_queries:
                    processed_query = sub_queries[0]
                    self.logger.info(f"Processed PubMed query: {processed_query}")
                else:
                    self.logger.warning("No PubMed sub-queries generated, skipping PubMed search")
                    continue  # å¦‚æœæ²¡æœ‰ç”Ÿæˆå­æŸ¥è¯¢ï¼Œè·³è¿‡å½“å‰æ£€ç´¢å™¨
            
            # å®ä¾‹åŒ–å½“å‰æ£€ç´¢å™¨
            retriever = retriever_class(processed_query, query_domains=query_domains)
            self.logger.info(f"*****use retriever: {current_retriever_type} search query: {processed_query}")
            # æ‰§è¡Œæœç´¢
            search_results = await asyncio.to_thread(
                retriever.search, max_results=self.researcher.cfg.max_search_results_per_query
            )
            self.logger.info(f"*****use retriever 2: {current_retriever_type}")
            ceshi_c = 0
            # ä¸ºæœç´¢ç»“æœæ·»åŠ æ£€ç´¢å™¨ç±»å‹æ ‡è¯†
            for result in search_results:
                result['retriever_type'] = current_retriever_type  # æ·»åŠ æ£€ç´¢å™¨ç±»å‹å­—æ®µ
                search_results_with_type.append(result)  # å°†ç»“æœæ·»åŠ åˆ°å¸¦æ£€ç´¢å™¨ç±»å‹çš„ç»“æœåˆ—è¡¨ä¸­
                ceshi_c += 1
                self.logger.info(f"æµ‹è¯•æ£€ç´¢å™¨ï¼š<{ceshi_c}>{current_retriever_type} \n æœç´¢ç»“æœï¼š{result['href']} ")
                # self.logger.info(f"æµ‹è¯•pubmedæœç´¢çš„å­—æ®µæ˜¯å¦å­˜åœ¨ï¼š0ï¼š{result.get('href')}")
                # self.logger.info(f"æµ‹è¯•pubmedæœç´¢çš„å­—æ®µæ˜¯å¦å­˜åœ¨ï¼š1ï¼š{result.get('published')}")
                # self.logger.info(f"æµ‹è¯•pubmedæœç´¢çš„å­—æ®µæ˜¯å¦å­˜åœ¨ï¼š2ï¼š{result.get('pagination')}")
                # self.logger.info(f"æµ‹è¯•pubmedæœç´¢çš„å­—æ®µæ˜¯å¦å­˜åœ¨ï¼š3ï¼š{result.get('authors')}")
                # self.logger.info(f"æµ‹è¯•pubmedæœç´¢çš„å­—æ®µæ˜¯å¦å­˜åœ¨ï¼š4ï¼š{result.get('vol')}")
                # self.logger.info(f"æµ‹è¯•pubmedæœç´¢çš„å­—æ®µæ˜¯å¦å­˜åœ¨ï¼š5ï¼š{result.get('title')}")

            # æ”¶é›†URL
            search_urls = [url.get("href") for url in search_results]
            new_search_urls.extend(search_urls)

        # è·å–æ–°çš„URLå¹¶éšæœºæ‰“ä¹±
        new_search_urls = await self._get_new_urls(new_search_urls)
        random.shuffle(new_search_urls)

        # è¿”å›URLåˆ—è¡¨å’Œå¸¦æ£€ç´¢å™¨ç±»å‹çš„ç»“æœ
        return new_search_urls, search_results_with_type

    async def _scrape_data_by_urls(self, sub_query, query_domains: list = []):
        """
        Runs a sub-query across multiple retrievers and scrapes the resulting URLs.

        Args:
            sub_query (str): The sub-query to search for.

        Returns:
            list: A list of scraped content results.
        """
        # è·å–URLåˆ—è¡¨å’Œå¸¦æ£€ç´¢å™¨ç±»å‹çš„ç»“æœ
        new_search_urls, search_results_with_type = await self._search_relevant_source_urls(sub_query, query_domains)

        # Log the research process if verbose mode is on
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "researching",
                f"ğŸ¤” Researching for relevant information across multiple sources...\n",
                self.researcher.websocket,
            )

        # Scrape the new URLs
        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)
        
        # è®°å½•æŠ“å–ç»“æœ
        self.logger.info(f"æŠ“å–ç»“æœæ•°é‡: {len(scraped_content)}")
        
        # åˆ›å»ºURLåˆ°æ£€ç´¢å™¨ç±»å‹çš„æ˜ å°„
        url_to_type = {}

        # åŠ ++
        # åˆå§‹åŒ–å…¶ä»–å­—æ®µçš„æ˜ å°„
        url_to_published_date = {}
        url_to_authors = {}
        url_to_vol = {}
        url_to_pagination = {}

        for result in search_results_with_type:
            if 'href' in result and 'retriever_type' in result:
                normalized_url = self._normalize_url(result['href'])
                url_to_type[normalized_url] = result['retriever_type']
                self.logger.info(f"æ·»åŠ æ£€ç´¢å™¨<-URLæ˜ å°„:{result['retriever_type']} <-- {normalized_url}")

                ## åŠ ++ä¿®++
                # if result['retriever_type'] == 'pubmed':  # å¦‚æœæ£€ç´¢å™¨ç±»å‹æ˜¯pubmed
                self.logger.info(f"pubmedæ£€ç´¢å™¨ç±»å‹, å¼€å§‹å¤„ç†pubmedå­—æ®µ")
                if 'published' in result:  # å¦‚æœç»“æœä¸­åŒ…å«å‘å¸ƒæ—¥æœŸå­—æ®µ
                    url_to_published_date[normalized_url] = result['published']  # æ·»åŠ åˆ°æ˜ å°„ä¸­
                    self.logger.info(f"æ·»åŠ å‘å¸ƒæ—¥æœŸæ˜ å°„: {normalized_url} -> {result['published']}")
                if 'authors' in result:  # å¦‚æœç»“æœä¸­åŒ…å«ä½œè€…å­—æ®µ
                    url_to_authors[normalized_url] = result['authors']  # æ·»åŠ åˆ°æ˜ å°„ä¸­
                    self.logger.info(f"æ·»åŠ ä½œè€…æ˜ å°„: {normalized_url} -> {result['authors']}")
                if 'vol' in result:  # å¦‚æœç»“æœä¸­åŒ…å«å·å·å­—æ®µ
                    url_to_vol[normalized_url] = result['vol']  # æ·»åŠ åˆ°æ˜ å°„ä¸­
                    self.logger.info(f"æ·»åŠ å·å·æ˜ å°„: {normalized_url} -> {result['vol']}")
                if 'pagination' in result:  # å¦‚æœç»“æœä¸­åŒ…å«åˆ†é¡µä¿¡æ¯å­—æ®µ
                    url_to_pagination[normalized_url] = result['pagination']  # æ·»åŠ åˆ°æ˜ å°„ä¸­
                    self.logger.info(f"æ·»åŠ åˆ†é¡µä¿¡æ¯æ˜ å°„: {normalized_url} -> {result['pagination']}")


        self.logger.info(f"å‘å¸ƒæ—¶é—´çš„æ˜ å°„å­—å…¸çš„é•¿åº¦æ˜¯ï¼š{len(url_to_published_date)}")

        # ä¸ºæ¯ä¸ªå†…å®¹æ·»åŠ æ£€ç´¢å™¨ç±»å‹ä¿¡æ¯
        for content in scraped_content:
            if 'url' in content:
                content_url = content['url']
                normalized_content_url = self._normalize_url(content_url)
                self.logger.info(f"å¤„ç†URL: {content_url}")
                self.logger.info(f"æ ‡å‡†åŒ–åçš„URL: {normalized_content_url}")
                
                # é¦–å…ˆå°è¯•ä»æ˜ å°„ä¸­è·å–ç±»å‹
                if normalized_content_url in url_to_type:
                    content['retriever_type'] = url_to_type[normalized_content_url]
                    self.logger.info(f"ä»URLæ˜ å°„ä¸­è·å–åˆ°æ£€ç´¢å™¨ç±»å‹: {content['retriever_type']}")
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ£€ç´¢å™¨ç±»å‹ï¼Œä½¿ç”¨URLç‰¹å¾åˆ¤æ–­
                    if 'arxiv' in normalized_content_url:
                        content['retriever_type'] = 'arxiv'
                        self.logger.info(f"æ ¹æ®URLç‰¹å¾åˆ¤æ–­ä¸ºarxiv")
                    elif 'ncbi' in normalized_content_url or 'pubmed' in normalized_content_url:
                        content['retriever_type'] = 'pubmed'
                        self.logger.info(f"æ ¹æ®URLç‰¹å¾åˆ¤æ–­ä¸ºpubmed")
                    else:
                        content['retriever_type'] = 'tavily'
                        self.logger.info(f"æ ¹æ®URLç‰¹å¾åˆ¤æ–­ä¸ºtavily")

                ## ä¿®++
                # ä¸ºæ¯ä¸ªå†…å®¹æ·»åŠ :1. å‡ºç‰ˆæ—¶é—´ published_date 2. æœŸåˆŠå·å· vol  3. ä½œè€…ä¿¡æ¯ authors 4. åˆ†é¡µä¿¡æ¯ pagination
                if normalized_content_url in url_to_published_date:  # å¦‚æœURLåœ¨å‘å¸ƒæ—¥æœŸæ˜ å°„ä¸­
                    content['published_date'] = url_to_published_date[normalized_content_url]  # æ·»åŠ å‘å¸ƒæ—¥æœŸå­—æ®µ
                    self.logger.info(f"æ·»åŠ å‘å¸ƒæ—¥æœŸ: {content['published_date']}")
                if normalized_content_url in url_to_authors:  # å¦‚æœURLåœ¨ä½œè€…æ˜ å°„ä¸­
                    content['authors'] = url_to_authors[normalized_content_url]  # æ·»åŠ ä½œè€…å­—æ®µ
                    self.logger.info(f"æ·»åŠ ä½œè€…: {content['authors']}")
                if normalized_content_url in url_to_vol:  # å¦‚æœURLåœ¨å·å·æ˜ å°„ä¸­
                    content['vol'] = url_to_vol[normalized_content_url]  # æ·»åŠ å·å·å­—æ®µ
                    self.logger.info(f"æ·»åŠ å·å·: {content['vol']}")
                if normalized_content_url in url_to_pagination:  # å¦‚æœURLåœ¨åˆ†é¡µä¿¡æ¯æ˜ å°„ä¸­
                    content['pagination'] = url_to_pagination[normalized_content_url]  # æ·»åŠ åˆ†é¡µä¿¡æ¯å­—æ®µ
                    self.logger.info(f"æ·»åŠ åˆ†é¡µä¿¡æ¯: {content['pagination']}")


        if self.researcher.vector_store:
            self.researcher.vector_store.load(scraped_content)

        return scraped_content

    def _normalize_url(self, url):
        """æ ‡å‡†åŒ–URLä»¥ä¾¿è¿›è¡ŒåŒ¹é…"""
        if not url:
            return ""
        # ç§»é™¤URLä¸­çš„åè®®å‰ç¼€
        url = re.sub(r'^https?://', '', url)
        # ç§»é™¤å°¾éƒ¨æ–œæ 
        url = url.rstrip('/')
        # ç§»é™¤URLå‚æ•°
        url = url.split('?')[0]
        # ç§»é™¤é”šç‚¹
        url = url.split('#')[0]
        return url.lower()

    async def _normalize_journal_name(self, name):
        """æ ‡å‡†åŒ–æœŸåˆŠåç§°ä»¥æé«˜åŒ¹é…æˆåŠŸç‡"""
        if not name:
            return name
            
        # è§£ç HTMLå®ä½“
        normalized = html.unescape(name)
        
        # æ ‡å‡†åŒ–ç©ºæ ¼
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # æ ‡å‡†åŒ–è¿å­—ç¬¦å’Œå¸¸è§æ›¿æ¢
        normalized = normalized.replace(' - ', '-')
        normalized = normalized.replace(' And ', ' & ')
        normalized = normalized.replace(' and ', ' & ')
        normalized = normalized.replace('&amp;', '&')
        normalized = normalized.replace('&AMP;', '&')
        
        # å¤„ç†å¸¸è§çš„æœŸåˆŠåç§°å˜ä½“
        replacements = {
            'J ': 'Journal ',
            'Intl': 'International',
            'Int ': 'International ',
            'Int.': 'International',
            'Rev ': 'Review ',
            'Rev.': 'Review',
            'Sci ': 'Science ',
            'Sci.': 'Science',
            'Adv ': 'Advances ',
            'Adv.': 'Advances',
            'Res ': 'Research ',
            'Res.': 'Research',
            'Chem ': 'Chemistry ',
            'Chem.': 'Chemistry',
            'Biol ': 'Biology ',
            'Biol.': 'Biology',
            'Med ': 'Medicine ',
            'Med.': 'Medicine',
        }
        
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        return normalized

    async def _extract_journal_info_from_url(self, url):
        """ä»URLä¸­æå–æœŸåˆŠä¿¡æ¯ï¼Œä¼˜å…ˆé€šè¿‡DOIè¯†åˆ«"""
        journal_info = {"journal_name": None}
        
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥ä»URLæå–DOI
            doi_match = re.search(r'(10\.\d{4,}[\/.][\w\.\-\/]+)', url)
            
            # å¦‚æœURLä¸­æ²¡æœ‰DOIï¼Œå°è¯•è®¿é—®é¡µé¢è·å–DOI
            if not doi_match:
                content = await self._fetch_url_content(url)
                if content:
                    # å°è¯•ä»HTMLä¸­æå–DOI
                    soup = BeautifulSoup(content, "html.parser")
                    
                    # æ£€æŸ¥å„ç§å¯èƒ½åŒ…å«DOIçš„å…ƒæ•°æ®æ ‡ç­¾
                    meta_doi = soup.find("meta", {"name": "citation_doi"}) or \
                            soup.find("meta", {"name": "dc.identifier"}) or \
                            soup.find("meta", {"name": "DC.Identifier"}) or \
                            soup.find("meta", {"scheme": "doi"})
                    
                    if meta_doi and meta_doi.get("content"):
                        doi = meta_doi.get("content")
                        if doi.startswith("doi:"):
                            doi = doi[4:]
                        doi_match = re.match(r'(10\.\d{4,}[\/.][\w\.\-\/]+)', doi)
                    else:
                        # å°è¯•ä»æ­£æ–‡ä¸­æŸ¥æ‰¾DOI
                        doi_regex = r'(?:doi|DOI):\s*(10\.\d{4,}[\/.][\w\.\-\/]+)'
                        content_match = re.search(doi_regex, content)
                        if content_match:
                            doi_match = re.match(r'(10\.\d{4,}[\/.][\w\.\-\/]+)', content_match.group(1))
            
            # å¦‚æœæ‰¾åˆ°DOIï¼Œé€šè¿‡CrossRef APIè·å–æœŸåˆŠä¿¡æ¯
            if doi_match:
                doi = doi_match.group(1)
                journal_info["doi"] = doi
                self.logger.info(f"Found DOI: {doi}")
                
                # è°ƒç”¨CrossRef API
                try:
                    crossref_api_url = f"https://api.crossref.org/works/{doi}"
                    response = await self._fetch_url_content(crossref_api_url)
                    
                    if response:
                        data = json.loads(response)
                        if "message" in data:
                            message = data["message"]
                            
                            # æå–æœŸåˆŠåç§°
                            if "container-title" in message and message["container-title"]:
                                journal_info["journal_name"] = message["container-title"][0]
                                self.logger.info(f"Found journal name from CrossRef: {journal_info['journal_name']}")
                            
                            # æå–ISSN (å¯ç”¨äºè¿›ä¸€æ­¥åŒ¹é…æœŸåˆŠå½±å“å› å­)
                            if "ISSN" in message and message["ISSN"]:
                                journal_info["issn"] = message["ISSN"][0]
                                
                            # æå–å‡ºç‰ˆå•†ä¿¡æ¯
                            if "publisher" in message:
                                journal_info["publisher"] = message["publisher"]
                                
                            return journal_info
                except Exception as e:
                    self.logger.warning(f"Error fetching CrossRef metadata: {e}")
            
            # å¦‚æœé€šè¿‡DOIæ— æ³•è·å–ï¼Œå›é€€åˆ°åŸºäºURLçš„æ–¹æ³•
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            path = parsed_url.path
            
            # å¤„ç†å¸¸è§å­¦æœ¯ç½‘ç«™çš„URLæ¨¡å¼
            # arXiv
            if "arxiv.org" in domain:
                journal_info["journal_name"] = "arXiv"
                return journal_info
                
            # PubMed/PMC
            if "pubmed.ncbi.nlm.nih.gov" in domain or "pmc.ncbi.nlm.nih.gov" in domain:
                if "pubmed.ncbi.nlm.nih.gov" in domain:
                    pmid_match = re.search(r'pubmed\.ncbi\.nlm\.nih\.gov\/(\d+)', url)
                    if pmid_match:
                        pmid = pmid_match.group(1)
                        try:
                            api_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id={pmid}&retmode=json"
                            response = await self._fetch_url_content(api_url)
                            if response:
                                data = json.loads(response)
                                if 'result' in data and pmid in data['result']:
                                    result = data['result'][pmid]
                                    if 'fulljournalname' in result:
                                        journal_info["journal_name"] = result['fulljournalname']
                                    elif 'source' in result:
                                        journal_info["journal_name"] = result['source']
                        except Exception as e:
                            self.logger.warning(f"Error fetching PubMed metadata: {e}")
                return journal_info
            
            # Nature
            if "nature.com" in domain:
                if "nature.com/articles/s41586" in url:
                    journal_info["journal_name"] = "Nature"
                elif "nature.com/articles/s41467" in url:
                    journal_info["journal_name"] = "Nature Communications"
                elif re.search(r'nature\.com\/([^\/]+)\/journal', url):
                    journal_match = re.search(r'nature\.com\/([^\/]+)\/journal', url)
                    if journal_match:
                        journal_name = journal_match.group(1).replace('-', ' ').title()
                        journal_info["journal_name"] = f"Nature {journal_name}"
                else:
                    # å°è¯•ä»så¼€å¤´çš„DOIé£æ ¼è·¯å¾„ä¸­æå–æœŸåˆŠæ ‡è¯†ç¬¦
                    s_match = re.search(r'nature\.com\/articles\/(s\d+)', url)
                    if s_match:
                        # NatureæœŸåˆŠDOIå‰ç¼€æ˜ å°„è¡¨
                        nature_prefixes = {
                            's41586': 'Nature',
                            's41467': 'Nature Communications',
                            's41598': 'Scientific Reports',
                            's41587': 'Nature Biotechnology',
                            's41591': 'Nature Medicine',
                            's41593': 'Nature Neuroscience',
                            's41594': 'Nature Structural & Molecular Biology',
                            's41561': 'Nature Geoscience',
                            's41563': 'Nature Materials',
                            's41565': 'Nature Nanotechnology',
                            's41566': 'Nature Photonics',
                            's41567': 'Nature Physics',
                            's41577': 'Nature Reviews Immunology',
                            's41573': 'Nature Reviews Drug Discovery',
                            's41574': 'Nature Reviews Endocrinology',
                            's41575': 'Nature Reviews Gastroenterology & Hepatology',
                            's41576': 'Nature Reviews Genetics',
                            's41577': 'Nature Reviews Immunology',
                            's41578': 'Nature Reviews Materials',
                            's41579': 'Nature Reviews Microbiology',
                            's41580': 'Nature Reviews Molecular Cell Biology',
                            's41581': 'Nature Reviews Nephrology',
                            's41582': 'Nature Reviews Neurology',
                            's41584': 'Nature Reviews Rheumatology',
                            's41571': 'Nature Reviews Clinical Oncology',
                            's42255': 'Nature Metabolism',
                            's42256': 'Nature Machine Intelligence',
                            's41567': 'Nature Physics',
                            's41557': 'Nature Chemistry',
                            's41558': 'Nature Climate Change',
                            's41559': 'Nature Ecology & Evolution',
                            's41560': 'Nature Energy',
                            's41564': 'Nature Microbiology',
                            's41570': 'Nature Reviews Chemistry',
                            's41589': 'Nature Chemical Biology',
                            's41592': 'Nature Methods',
                            's41596': 'Nature Protocols',
                            's41597': 'Scientific Data',
                            's41598': 'Scientific Reports',
                            's41699': 'Communications Biology',
                            's41746': 'npj Digital Medicine',
                            's42003': 'Communications Biology',
                        }
                        
                        s_id = s_match.group(1)
                        for prefix, journal in nature_prefixes.items():
                            if s_id.startswith(prefix):
                                journal_info["journal_name"] = journal
                                break
                
                return journal_info
            
            # BioMedCentral
            if "biomedcentral.com" in domain:
                journal_subdomain = domain.split(".biomedcentral.com")[0]
                bmc_journals = {
                    "ann-clinmicrob": "Annals of Clinical Microbiology and Antimicrobials",
                    "bmcgenomics": "BMC Genomics",
                    "bmcinfectdis": "BMC Infectious Diseases",
                    "bmcmicrobiol": "BMC Microbiology",
                    "genomemedicine": "Genome Medicine",
                    "translationalmedicine": "Journal of Translational Medicine",
                    "bmcbiol": "BMC Biology",
                    "bmccancer": "BMC Cancer",
                    "bmcneurosci": "BMC Neuroscience",
                    "bmcpsychiatry": "BMC Psychiatry",
                    "jnanobiotechnology": "Journal of Nanobiotechnology",
                    "mbio": "mBio",
                    "microbiome": "Microbiome",
                    "parasitesandvectors": "Parasites & Vectors",
                    "retrovirology": "Retrovirology",
                    "virologyj": "Virology Journal",
                }
                
                if journal_subdomain in bmc_journals:
                    journal_info["journal_name"] = bmc_journals[journal_subdomain]
                
                return journal_info
            
            # Lancet
            if "thelancet.com" in domain:
                if "thelancet.com/journals/lancet" in url:
                    journal_info["journal_name"] = "The Lancet"
                else:
                    lancet_match = re.search(r'thelancet\.com\/journals\/([^\/]+)', url)
                    if lancet_match:
                        journal_code = lancet_match.group(1)
                        lancet_journals = {
                            "laninf": "The Lancet Infectious Diseases",
                            "lanmic": "The Lancet Microbe",
                            "lanepe": "The Lancet Regional Health - Europe",
                            "lanpsy": "The Lancet Psychiatry",
                            "landia": "The Lancet Diabetes & Endocrinology",
                            "langas": "The Lancet Gastroenterology & Hepatology",
                            "lanhae": "The Lancet Haematology",
                            "lanhiv": "The Lancet HIV",
                            "laneur": "The Lancet Neurology",
                            "lanplh": "The Lancet Planetary Health",
                            "lanpub": "The Lancet Public Health",
                            "lanres": "The Lancet Respiratory Medicine",
                            "lanrhe": "The Lancet Rheumatology",
                            "lanonc": "The Lancet Oncology",
                            "lancet": "The Lancet",
                            "lanchi": "The Lancet Child & Adolescent Health",
                            "landig": "The Lancet Digital Health",
                            "eclinm": "EClinicalMedicine",
                        }
                        if journal_code in lancet_journals:
                            journal_info["journal_name"] = lancet_journals[journal_code]
                
                return journal_info
            
            # MDPI
            if "mdpi.com" in domain:
                mdpi_match = re.search(r'mdpi\.com\/journal\/([^\/]+)', url)
                if mdpi_match:
                    journal_name = mdpi_match.group(1).replace('-', ' ').title()
                    journal_info["journal_name"] = journal_name
                else:
                    # å°è¯•ä»URLæå–ISSN
                    issn_match = re.search(r'mdpi\.com\/(\d{4}-\d{3}[\dX])', url)
                    if issn_match:
                        journal_info["issn"] = issn_match.group(1)
                
                return journal_info
            
            # Frontiers
            if "frontiersin.org" in domain:
                frontiers_match = re.search(r'frontiersin\.org\/journals\/([^\/]+)', url)
                if frontiers_match:
                    journal_slug = frontiers_match.group(1).replace('-', ' ')
                    journal_info["journal_name"] = f"Frontiers in {journal_slug.title()}"
                
                return journal_info
            
            # RSC (Royal Society of Chemistry)
            if "rsc.org" in domain or "pubs.rsc.org" in domain:
                rsc_match = re.search(r'\/([^\/]+)\/article', url)
                if rsc_match:
                    journal_code = rsc_match.group(1)
                    rsc_journals = {
                        "c0": "Chemical Communications",
                        "cc": "Chemical Communications",
                        "cs": "Chemical Science",
                        "dt": "Dalton Transactions",
                        "gc": "Green Chemistry",
                        "cp": "Chemical Physics",
                        "sc": "Sustainable Chemistry",
                        "an": "Analyst",
                        "ce": "Chemical Education",
                        "cy": "Catalysis Science & Technology",
                        "ee": "Energy & Environmental Science",
                        "en": "Environmental Science",
                        "fd": "Faraday Discussions",
                        "lc": "Lab on a Chip",
                        "nr": "Nanoscale",
                        "ob": "Organic & Biomolecular Chemistry",
                        "py": "Polymer Chemistry",
                        "ra": "RSC Advances",
                        "sm": "Soft Matter",
                    }
                    if journal_code in rsc_journals:
                        journal_info["journal_name"] = rsc_journals[journal_code]
                
                return journal_info
            
            # å¦‚æœä»æ— æ³•ç¡®å®šæœŸåˆŠï¼Œå°è¯•ä»å†…å®¹ä¸­æå–
            if not journal_info["journal_name"] and not content:
                content = await self._fetch_url_content(url)
                
            if content and not journal_info["journal_name"]:
                soup = BeautifulSoup(content, "html.parser")
                
                # å°è¯•ä»å…ƒæ•°æ®ä¸­æå–æœŸåˆŠå
                journal_meta = soup.find("meta", {"name": "citation_journal_title"}) or \
                            soup.find("meta", {"name": "prism.publicationName"}) or \
                            soup.find("meta", {"name": "dc.source"}) or \
                            soup.find("meta", {"property": "og:site_name"})
                            
                if journal_meta and journal_meta.get("content"):
                    journal_info["journal_name"] = journal_meta.get("content")
                    self.logger.info(f"Found journal name from HTML metadata: {journal_info['journal_name']}")
        # æ ‡å‡†åŒ–æœŸåˆŠåç§°
            if journal_info["journal_name"]:
                journal_info["journal_name"] = await self._normalize_journal_name(journal_info["journal_name"])
                self.logger.info(f"Normalized journal name: {journal_info['journal_name']}")
                              
            return journal_info
            
        except Exception as e:
            self.logger.warning(f"Error extracting journal info from URL: {e}")
            return journal_info

    async def _fetch_url_content(self, url, timeout=10):
        """è·å–URLå†…å®¹çš„è¾…åŠ©å‡½æ•°"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = await asyncio.to_thread(
                lambda: requests.get(url, headers=headers, timeout=timeout)
            )
            
            if response.status_code == 200:
                return response.text
            return None
        except Exception as e:
            self.logger.warning(f"Error fetching URL {url}: {e}")
            return None


    #! åŠ ++
    def _extract_arxiv_info_from_url(self, url):
        """ä»arXiv URLä¸­æå–ä¿¡æ¯"""
        try:
            arxiv_info = {}
            # ä»URLä¸­æå–arXiv ID
            arxiv_id_match = re.search(r'arxiv\.org/pdf/(\d{4})\.(.*?)$', url)
            if not arxiv_id_match:
                arxiv_id_match = re.search(r'\.([^./]+)$', url)  # åŒ¹é…æœ€åä¸€ä¸ªç‚¹åéæ–œæ å†…å®¹
                arxiv_id = arxiv_id_match.group(1)
                if not arxiv_id_match:
                    arxiv_id = "Unknown document"  # æ— æ³•æå–arxiv IDï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            else:
                arxiv_id = arxiv_id_match.group(2)

            arxiv_info["arxiv_id"] = arxiv_id
            arxiv_info["published_date"] = self.extract_and_convert_arxiv_date(url)

            return arxiv_info

        except Exception as e:
            self.logger.warning(f"Error extracting arXiv info from URL: {e}")

    def extract_and_convert_arxiv_date(self,url: str) -> str:
        """
        ä»arXivé“¾æ¥ä¸­æå–æ—¥æœŸæ ‡è®°å¹¶è½¬æ¢ä¸ºæœŸå¾…æ—¥æœŸæ ¼å¼

        å‚æ•°:
        url (str): arXivè®ºæ–‡é“¾æ¥ï¼Œå¦‚"http://arxiv.org/pdf/2410.15367v1"

        è¿”å›:
        str: æœŸå¾…æ—¥æœŸæ ¼å¼ï¼šå¦‚"2024.10"
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…arxiv IDä¸­çš„æ—¥æœŸéƒ¨åˆ†ï¼ˆå¦‚2410æˆ–1704ï¼‰
        match = re.search(r'arxiv\.org/pdf/(\d{2})(\d{2})', url, re.IGNORECASE)
        if not match:
            match = re.search(r'\.(\d{2})(\d{2})', url[::-1])
            if match:
                digits_m = match.group(1)[::-1]
                digits_y = match.group(2)[::-1]
                digits = digits_y + digits_m  # åˆå¹¶å¹´å’Œæœˆ
            else:
                return ""  # æ— æ³•æå–æ—¥æœŸï¼Œè¿”å›åŸURL
        else:
            # æå–å¹´ä»½å’Œæœˆä»½
            digits_y, digits_m = match.groups()

        # å°†2ä½å¹´ä»½è½¬æ¢ä¸º4ä½ï¼ˆå‡è®¾2000å¹´ä¹‹åï¼‰
        year = int(digits_y)
        year_full = 2000 + year if year < 50 else 1900 + year

        # æ„å»ºæœŸå¾…æ—¥æœŸ
        try:
            # ç¡®ä¿æœˆä»½åœ¨æœ‰æ•ˆèŒƒå›´ï¼ˆ1-12ï¼‰
            month = int(digits_m)
            if 1 <= month <= 12:
                # return f"{year_full}.{month:02d}"  # ç›´æ¥æ ¼å¼åŒ–å­—ç¬¦ä¸²
                return f"{year_full}"
            else:
                return f"{year_full}"  # æ— æ•ˆæœˆä»½ï¼Œåªå‘å¹´ä»½
        except ValueError:
            return f"{year_full}-{digits_m.zfill(2)}"  # å¼‚å¸¸å¤„ç†ï¼Œä¿æŒåŸå§‹æ•°å­—